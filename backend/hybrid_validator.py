import numpy as np
import re
from typing import List, Dict, Tuple
from rank_bm25 import BM25Okapi

class HybridValidator:
    def __init__(self, embedding_model):
        self.model = embedding_model
        # Umbrales para clasificaciÃ³n de respuestas (basados en Short Answer Grading - SAG)
        # Estos umbrales se aplican sobre S_raw (score bruto en [0,1])
        self.thresholds = {
            'excelente': 0.85,   # â‰¥0.85 â†’ Excelente (90-100%)
            'bueno': 0.70,       # 0.70-0.84 â†’ Bueno (70-89%)
            'aceptable': 0.50,   # 0.50-0.69 â†’ Aceptable (50-69%)
            'rechazo': 0.50      # <0.50 â†’ Necesita mejorar (0-49%)
        }
        # Pesos optimizados para OCR + parafraseo (basado en literatura SAG)
        # Priorizan semÃ¡ntica sobre lÃ©xico por errores OCR en PDFs
        self.weights = {
            'bm25': 0.05,        # 5% - Coincidencias lÃ©xicas (reducido por OCR)
            'cosine': 0.80,      # 80% - Similitud semÃ¡ntica (eje principal)
            'coverage': 0.15     # 15% - Cobertura de keywords clave
        }
        # Rango de normalizaciÃ³n para cosine similarity (valores empÃ­ricos)
        # AJUSTADO: Para PDFs con texto corrupto/OCR, los cosines son mÃ¡s bajos
        # Basado en all-MiniLM-L6-v2 + anÃ¡lisis de respuestas reales
        self.cosine_min = 0.25   # Por debajo: casi siempre incorrecto (antes: 0.30)
        self.cosine_max = 0.70   # Por encima: muy similar al texto (antes: 0.80)
        
        # Min-max scaling para porcentaje 0-100%
        # AJUSTADO: MÃ¡s tolerante para texto con errores de espaciado
        self.expected_min = 0.25  # Respuesta muy mala â†’ 0% (antes: 0.30)
        self.expected_max = 0.85  # Respuesta excelente â†’ 100% (antes: 0.90)
        
        self.stopwords = {'el', 'la', 'los', 'las', 'un', 'una', 'de', 'del', 'a', 'al', 'en', 'por', 'para', 'con', 'y', 'o', 'pero', 'si', 'no', 'que', 'como', 'cuando', 'donde', 'cual', 'quien', 'su', 'sus', 'mi', 'mis', 'tu', 'tus', 'se', 'le', 'lo', 'me', 'te', 'nos', 'os'}
    
    def normalize_cosine(self, cosine_sim: float) -> float:
        """
        Normaliza similitud del coseno al rango 0-1 usando lÃ­mites empÃ­ricos
        
        Basado en investigaciÃ³n de Sentence-BERT:
        - cosine < 0.30: sin relaciÃ³n (0%)
        - cosine = 0.80: muy similar (100%)
        
        Referencia: Reimers & Gurevych (2019), "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
        """
        normalized = (cosine_sim - self.cosine_min) / (self.cosine_max - self.cosine_min)
        return max(0.0, min(1.0, normalized))  # Clamp a [0, 1]
    
    def length_bonus(self, answer: str) -> float:
        """
        Bonus pequeÃ±o por respuestas de longitud razonable
        Evita respuestas telegrama o testamentos
        
        Args:
            answer: Texto de la respuesta del alumno
            
        Returns:
            float: Bonus en [0, 0.05] (mÃ¡ximo 5 puntos extra)
        """
        tokens = len(answer.split())
        if 8 <= tokens <= 80:  # Rango razonable para Active Recall
            return 0.05
        return 0.0
    
    def to_percentage(self, score_raw: float) -> float:
        """
        Convierte score bruto [0,1] a porcentaje 0-100% con min-max scaling
        
        Mapea el rango esperado de scores reales [0.30-0.90] a [0-100%]
        para distribuir mejor los resultados en toda la escala.
        
        Args:
            score_raw: Score bruto en [0,1]
            
        Returns:
            float: Porcentaje en [0.0, 100.0]
        """
        scaled = (score_raw - self.expected_min) / (self.expected_max - self.expected_min)
        scaled = max(0.0, min(1.0, scaled))  # Clamp a [0,1]
        return round(scaled * 100, 1)
    
    def is_inferential_question(self, question: str) -> bool:
        """
        Detecta si una pregunta es inferencial por palabras clave.
        
        BASADO EN: TaxonomÃ­a de Bloom + InvestigaciÃ³n en Reading Comprehension
        
        NIVELES DE COMPRENSIÃ“N (Barrett, 1968; Bloom, 1956):
        
        1. LITERAL: Recordar informaciÃ³n explÃ­cita del texto
           - "Â¿QuiÃ©n...?", "Â¿CuÃ¡ndo...?", "Â¿DÃ³nde...?", "Â¿QuÃ© dijo...?"
           
        2. INFERENCIAL: Deducir informaciÃ³n implÃ­cita
           - "Â¿Por quÃ©...?", "Â¿QuÃ© sugiere...?", "Â¿CÃ³mo se explica...?"
           
        3. CRÃTICO/EVALUATIVO: Juzgar y valorar
           - "Â¿QuÃ© opinas...?", "Â¿EstÃ¡s de acuerdo...?"
        
        Las preguntas inferenciales requieren que el lector:
        - Conecte informaciÃ³n de diferentes partes del texto
        - Use conocimiento previo + informaciÃ³n del texto
        - Haga deducciones lÃ³gicas
        
        Referencia: Education Endowment Foundation (2025), 
        "Reading Comprehension Strategies" - Making inferences
        
        Args:
            question: Texto de la pregunta
            
        Returns:
            bool: True si es inferencial, False si es literal
        """
        # Keywords para preguntas INFERENCIALES (requieren razonamiento)
        inferential_keywords = [
            # Causales - "Â¿Por quÃ©?"
            'por quÃ©', 'por que', 'cuÃ¡l es la razÃ³n', 'cual es la razon',
            'quÃ© razones', 'que razones', 'quÃ© motivo', 'que motivo',
            'a quÃ© se debe', 'a que se debe', 'cÃ³mo se explica', 'como se explica',
            
            # Inferenciales - DeducciÃ³n
            'quÃ© sugiere', 'que sugiere', 'quÃ© indica', 'que indica',
            'quÃ© permite deducir', 'que permite deducir', 'quÃ© implica', 'que implica',
            'quÃ© evidencia', 'que evidencia', 'quÃ© indicio', 'que indicio',
            'cÃ³mo deduce', 'como deduce', 'cÃ³mo sabes', 'como sabes',
            'quÃ© puedes inferir', 'que puedes inferir',
            'quÃ© conclusiÃ³n', 'que conclusion',
            
            # Predictivas
            'quÃ© pasarÃ­a si', 'que pasaria si', 'quÃ© crees que', 'que crees que',
            'quÃ© hubiera pasado', 'que hubiera pasado',
            
            # Comparativas/AnalÃ­ticas
            'en quÃ© se diferencia', 'en que se diferencia',
            'quÃ© relaciÃ³n', 'que relacion', 'cÃ³mo se relaciona', 'como se relaciona',
            'quÃ© tienen en comÃºn', 'que tienen en comun',
            
            # Intenciones/PropÃ³sito
            'con quÃ© intenciÃ³n', 'con que intencion',
            'para quÃ©', 'para que', 'cuÃ¡l es el propÃ³sito', 'cual es el proposito',
            'quÃ© pretende', 'que pretende',
            
            # Significado/Simbolismo (NUEVO)
            'quÃ© representa', 'que representa', 'quÃ© simboliza', 'que simboliza',
            'quÃ© significa', 'que significa', 'cuÃ¡l es el significado', 'cual es el significado',
            'quÃ© importancia', 'que importancia', 'cuÃ¡l es la importancia', 'cual es la importancia',
            'quÃ© papel', 'que papel', 'quÃ© rol', 'que rol',
            'quÃ© sentido', 'que sentido'
        ]
        
        question_lower = question.lower()
        return any(keyword in question_lower for keyword in inferential_keywords)
    
    def detect_contradiction(self, user_answer: str, chunk_text: str, question: str = "") -> Tuple[bool, float, str]:
        """
        Detecta si la respuesta del usuario CONTRADICE el contenido del chunk.
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        PROBLEMA IDENTIFICADO (Testing con GPT):
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        Ejemplo de fallo sin esta funciÃ³n:
        - Chunk dice: "Henriette recibÃ­a dinero cada aÃ±o por correo"
        - Usuario responde: "La condesa nunca le mandÃ³ dinero"
        - Sin NLI: score alto (78%) porque comparte palabras como "dinero", "correo"
        - Con NLI: deberÃ­a ser RECHAZO (<50%)
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ESTRATEGIA DE DETECCIÃ“N:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        1. Identificar KEYWORDS CLAVE del chunk (sustantivos importantes)
        2. Buscar patrones de NEGACIÃ“N en la respuesta del usuario
        3. Si encuentra negaciÃ³n de keyword clave â†’ CONTRADICCIÃ“N
        
        Patrones de negaciÃ³n:
        - "no [keyword]", "nunca [keyword]", "sin [keyword]"
        - "jamÃ¡s", "ningÃºn/ninguna", "nada de"
        
        Args:
            user_answer: Respuesta del usuario
            chunk_text: Texto del chunk de referencia
            question: Pregunta original (para contexto)
            
        Returns:
            Tuple[bool, float, str]: (is_contradiction, penalty_factor, reason)
        """
        answer_lower = user_answer.lower()
        chunk_lower = chunk_text.lower()
        
        # Keywords importantes que si se niegan = contradicciÃ³n grave
        # Agrupados por categorÃ­a semÃ¡ntica
        critical_keywords = {
            # Dinero/EconomÃ­a
            'dinero': ['dinero', 'francos', 'billetes', 'moneda', 'plata', 'efectivo', 'suma', 'pago'],
            'envio': ['enviÃ³', 'enviaba', 'enviar', 'mandÃ³', 'mandaba', 'mandar', 'recibÃ­a', 'recibiÃ³'],
            'correo': ['correo', 'carta', 'cartas', 'sobre', 'sobres', 'correspondencia'],
            # Personas/Acciones
            'ayuda': ['ayuda', 'ayudÃ³', 'ayudaba', 'asistencia', 'apoyo'],
            'robo': ['robÃ³', 'robar', 'hurto', 'ladrÃ³n', 'robado'],
            'culpa': ['culpable', 'inocente', 'sospechoso', 'acusado'],
        }
        
        # Patrones de negaciÃ³n en espaÃ±ol
        # MEJORADO: Permite hasta 4 palabras entre la negaciÃ³n y el keyword
        # Ejemplo: "nunca le mandÃ³ dinero" â†’ detecta "nunca ... dinero"
        negation_patterns = [
            # Patrones directos (negaciÃ³n + keyword)
            r'\bno\s+{keyword}\b',
            r'\bnunca\s+{keyword}\b',
            r'\bjamÃ¡s\s+{keyword}\b',
            r'\bsin\s+{keyword}\b',
            r'\bningÃºn\s+{keyword}\b',
            r'\bninguna\s+{keyword}\b',
            # Patrones con 1-2 palabras intermedias
            r'\bno\s+\w+\s+{keyword}\b',
            r'\bnunca\s+\w+\s+{keyword}\b',
            r'\bno\s+le\s+\w+\s+{keyword}\b',
            r'\bnunca\s+le\s+\w+\s+{keyword}\b',
            # Patrones con 2-3 palabras intermedias (ej: "nunca le mandÃ³ dinero")
            r'\bno\s+\w+\s+\w+\s+{keyword}\b',
            r'\bnunca\s+\w+\s+\w+\s+{keyword}\b',
            r'\bjamÃ¡s\s+\w+\s+\w+\s+{keyword}\b',
            # Patrones compuestos con "pero"
            r'\bpero\s+no\s+\w*\s*{keyword}\b',
            r'\bpero\s+nunca\s+\w*\s*{keyword}\b',
            r'\bpero\s+nunca\s+\w+\s+\w+\s+{keyword}\b',
            # PatrÃ³n genÃ©rico: negaciÃ³n seguida de hasta 4 palabras + keyword
            r'\b(no|nunca|jamÃ¡s|sin)\s+(?:\w+\s+){0,4}{keyword}\b',
        ]
        
        contradictions_found = []
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ESTRATEGIA 1: ContradicciÃ³n con el CHUNK
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        for category, keywords in critical_keywords.items():
            for keyword in keywords:
                # Verificar si el keyword estÃ¡ en el chunk (es relevante)
                if keyword in chunk_lower:
                    # Buscar si el usuario NIEGA este keyword
                    for pattern_template in negation_patterns:
                        pattern = pattern_template.format(keyword=keyword)
                        if re.search(pattern, answer_lower):
                            contradictions_found.append({
                                'category': category,
                                'keyword': keyword,
                                'pattern': pattern,
                                'source': 'chunk'
                            })
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ESTRATEGIA 2: ContradicciÃ³n con la PREGUNTA
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Si la pregunta habla de "ayuda econÃ³mica" y la respuesta dice
        # "nunca le mandÃ³ dinero", es contradicciÃ³n aunque el chunk no tenga "dinero"
        question_lower = question.lower() if question else ""
        
        # Detectar tema de la pregunta
        question_topics = {
            'economico': ['ayuda econÃ³mica', 'dinero', 'econÃ³mica', 'francos', 'billetes', 'pago', 'suma'],
            'envio': ['recibÃ­a', 'enviaba', 'mandaba', 'por correo', 'carta'],
        }
        
        for topic, topic_keywords in question_topics.items():
            # Si la pregunta trata sobre este tema
            if any(tk in question_lower for tk in topic_keywords):
                # Buscar si la respuesta NIEGA keywords relacionados con el tema
                related_keywords = critical_keywords.get(topic, []) + critical_keywords.get('dinero', [])
                for keyword in related_keywords:
                    for pattern_template in negation_patterns:
                        pattern = pattern_template.format(keyword=keyword)
                        if re.search(pattern, answer_lower):
                            # Evitar duplicados
                            if not any(c['keyword'] == keyword and c['source'] == 'question' for c in contradictions_found):
                                contradictions_found.append({
                                    'category': topic,
                                    'keyword': keyword,
                                    'pattern': pattern,
                                    'source': 'question'
                                })
        
        if contradictions_found:
            # Cuantas mÃ¡s contradicciones, mayor penalizaciÃ³n
            num_contradictions = len(contradictions_found)
            
            if num_contradictions >= 3:
                penalty = 0.25  # Reducir score al 25%
                severity = "GRAVE"
            elif num_contradictions >= 2:
                penalty = 0.35  # Reducir score al 35%
                severity = "MODERADA"
            else:
                penalty = 0.50  # Reducir score al 50%
                severity = "LEVE"
            
            keywords_negated = [c['keyword'] for c in contradictions_found[:3]]
            reason = f"ContradicciÃ³n {severity}: negaciÃ³n de '{', '.join(keywords_negated)}'"
            
            print(f"   âš ï¸ CONTRADICCIÃ“N DETECTADA: {reason}")
            print(f"   ğŸ“‰ PenalizaciÃ³n: score Ã— {penalty}")
            
            return True, penalty, reason
        
        return False, 1.0, ""
    
    def apply_pedagogical_boost(self, score_raw: float, cosine: float, 
                                user_answer: str, ref_text: str, question: str = "") -> float:
        """
        Booster pedagÃ³gico para respuestas de comprensiÃ³n lectora.
        
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        BASADO EN INVESTIGACIÃ“N:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        1. Sentence-BERT (Reimers & Gurevych, 2019):
           - Cosine similarity en embeddings tiene rango efectivo [0.3-0.8]
           - Por debajo de 0.3: sin relaciÃ³n semÃ¡ntica
           - Por encima de 0.6: alta similitud
           
        2. Short Answer Grading (Lloyd et al., 2022):
           - Las respuestas parafraseadas son tan vÃ¡lidas como las literales
           - El contenido semÃ¡ntico importa mÃ¡s que las palabras exactas
           
        3. Reading Comprehension Strategies (EEF, 2025):
           - Preguntas inferenciales requieren conectar ideas
           - La respuesta correcta puede NO contener palabras del texto
           
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ESTRATEGIA DE BOOST:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        A) LITERAL (recuerdo directo):
           - Threshold: cosine >= 0.40
           - Boost: x1.3 si respuesta concisa
           
        B) INFERENCIAL (razonamiento):
           - Threshold: cosine >= 0.35 (mÃ¡s flexible!)
           - Boost: x1.4 (reconoce esfuerzo cognitivo mayor)
           - El usuario puede usar palabras propias
        
        Args:
            score_raw: Score antes del boost [0,1]
            cosine: Similitud de embeddings normalizada [0,1]
            user_answer: Texto de la respuesta del usuario
            ref_text: Texto del chunk de referencia
            question: Texto de la pregunta (para detectar tipo)
            
        Returns:
            float: Score despuÃ©s del boost (mÃ¡x 0.99)
        """
        is_inferential = question and self.is_inferential_question(question)
        
        # Umbrales diferenciados segÃºn tipo de pregunta
        if is_inferential:
            # Preguntas inferenciales: mÃ¡s tolerantes
            # El usuario usa sus propias palabras para explicar
            BASE_THRESHOLD = 0.30      # MÃ¡s bajo: permite parafraseo
            BOOST_THRESHOLD = 0.35     # Umbral para boost
            BOOST_FACTOR = 1.45        # Boost mÃ¡s alto: reconoce razonamiento
            print(f"   ğŸ§  Pregunta INFERENCIAL detectada - umbral flexible")
        else:
            # Preguntas literales: estÃ¡ndar
            BASE_THRESHOLD = 0.40
            BOOST_THRESHOLD = 0.45
            BOOST_FACTOR = 1.30
        
        # Si estÃ¡ por debajo del umbral base, no hay boost
        if cosine < BASE_THRESHOLD:
            return score_raw
        
        # Calcular ratio de longitud (palabras)
        len_user = max(len(user_answer.split()), 1)
        len_ref = max(len(ref_text.split()), 1)
        len_ratio = len_user / len_ref
        
        boosted = score_raw
        
        # Boost 1: Respuestas concisas pero correctas (sÃ­ntesis)
        if len_ratio < 0.5 and cosine >= BOOST_THRESHOLD:
            # El usuario sintetizÃ³ bien la informaciÃ³n
            factor = 1.5 if len_ratio < 0.3 else 1.35
            boosted = score_raw * factor
            print(f"   ğŸ“ Boost sÃ­ntesis aplicado: x{factor:.2f}")
        
        # Boost 2: Preguntas inferenciales con respuesta razonada
        elif is_inferential and cosine >= BOOST_THRESHOLD:
            # El usuario demostrÃ³ comprensiÃ³n profunda
            boosted = score_raw * BOOST_FACTOR
            print(f"   ğŸ¯ Boost inferencial aplicado: x{BOOST_FACTOR:.2f}")
        
        # Limitar a 99% mÃ¡ximo (nunca dar 100% automÃ¡ticamente)
        return min(boosted, 0.99)
    
    def normalize_embedding(self, embedding: np.ndarray) -> np.ndarray:
        norm = np.linalg.norm(embedding)
        if norm < 1e-10:  # Threshold para evitar division por cero o numeros muy pequeÃ±os
            return embedding
        return embedding / norm
    
    def extract_keywords(self, text: str):
        # Normalizar texto antes de extraer keywords (quitar espacios OCR)
        # Ejemplo: "H enriet te" â†’ "Henriette"
        text = re.sub(r'\b(\w{1,2})\s+(\w{1,2})\b', r'\1\2', text)
        for _ in range(3):
            text = re.sub(r'\b(\w{1,2})\s+(\w{1,2})\b', r'\1\2', text)
        text = re.sub(r'\b(\w{2,4})\s+(\w{3,6})\b', r'\1\2', text)
        
        text = text.lower()
        words = re.findall(r'\b\w{3,}\b', text)
        keywords = [w for w in words if w not in self.stopwords]
        return keywords
    
    def expand_keywords(self, keywords):
        expanded = set(keywords)
        for word in keywords:
            expanded.add(word)
            if len(word) >= 6:
                expanded.add(word[:6])
            elif len(word) >= 5:
                expanded.add(word[:5])
            if word[0].isupper():
                expanded.add(word.lower())
        return expanded
    
    def bm25_score(self, query_keywords, chunk_text: str, corpus):
        tokenized_corpus = [self.extract_keywords(text) for text in corpus]
        
        # ProtecciÃ³n: si el corpus estÃ¡ vacÃ­o o todos los documentos vacÃ­os
        if not tokenized_corpus or all(len(doc) == 0 for doc in tokenized_corpus):
            return 0.0
        
        bm25 = BM25Okapi(tokenized_corpus)
        expanded_query = list(self.expand_keywords(query_keywords))
        
        # ProtecciÃ³n: si la query estÃ¡ vacÃ­a
        if not expanded_query:
            return 0.0
        
        scores = bm25.get_scores(expanded_query)
        chunk_keywords = self.extract_keywords(chunk_text)
        try:
            chunk_index = tokenized_corpus.index(chunk_keywords)
            return scores[chunk_index]
        except ValueError:
            return np.mean(scores) if len(scores) > 0 else 0.0
    
    def cosine_similarity(self, emb1: np.ndarray, emb2: np.ndarray) -> float:
        emb1_norm = self.normalize_embedding(emb1)
        emb2_norm = self.normalize_embedding(emb2)
        similarity = np.dot(emb1_norm, emb2_norm)
        return max(0.0, min(1.0, similarity))
    
    def calculate_coverage(self, answer_keywords, chunk_keywords):
        answer_expanded = self.expand_keywords(answer_keywords)
        chunk_expanded = self.expand_keywords(chunk_keywords)
        intersection = answer_expanded & chunk_expanded
        if len(answer_expanded) == 0:
            return 0.0
        coverage = len(intersection) / len(answer_expanded)
        return coverage
    
    def hybrid_score(self, question: str, answer: str, chunk, all_chunks):
        question_keywords = self.extract_keywords(question)
        answer_keywords = self.extract_keywords(answer)
        combined_keywords = list(set(question_keywords + answer_keywords))
        
        answer_embedding = self.normalize_embedding(
            self.model.encode(answer, convert_to_tensor=False)
        )
        chunk_embedding = self.normalize_embedding(
            np.array(chunk['embedding'])
        )
        
        corpus = [c['text_full'] for c in all_chunks]
        bm25_score_raw = self.bm25_score(combined_keywords, chunk['text_full'], corpus)
        bm25_normalized = min(1.0, bm25_score_raw / 10.0)
        
        cosine_score_raw = self.cosine_similarity(answer_embedding, chunk_embedding)
        # NUEVO: Normalizar cosine al rango 0-1 basado en valores empÃ­ricos
        cosine_normalized = self.normalize_cosine(cosine_score_raw)
        
        chunk_keywords = self.extract_keywords(chunk['text_full'])
        coverage_score = self.calculate_coverage(answer_keywords, chunk_keywords)
        
        # Score base: combinar mÃ©tricas normalizadas con pesos calibrados
        # 80% semÃ¡ntica + 15% cobertura + 5% lÃ©xico (reducido por OCR)
        score_base = (
            self.weights['bm25'] * bm25_normalized +
            self.weights['cosine'] * cosine_normalized +
            self.weights['coverage'] * coverage_score
        )
        
        # Aplicar bonus por longitud razonable (+5% mÃ¡ximo)
        bonus = self.length_bonus(answer)
        score_raw = max(0.0, min(1.0, score_base + bonus))  # Clamp a [0,1]
        
        # NUEVO: Aplicar boost pedagÃ³gico para respuestas concisas pero correctas
        # + boost inferencial para preguntas de razonamiento
        score_raw = self.apply_pedagogical_boost(
            score_raw=score_raw,
            cosine=cosine_normalized,
            user_answer=answer,
            ref_text=chunk['text_full'],
            question=question  # Para detectar preguntas inferenciales
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # NUEVO: DetecciÃ³n de contradicciÃ³n (NLI simplificado)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Si el usuario NIEGA conceptos clave del chunk, penalizar fuertemente
        # Ejemplo: chunk dice "recibÃ­a dinero" â†’ usuario dice "nunca le mandÃ³ dinero"
        is_contradiction, penalty_factor, contradiction_reason = self.detect_contradiction(
            user_answer=answer,
            chunk_text=chunk['text_full'],
            question=question
        )
        
        if is_contradiction:
            score_raw = score_raw * penalty_factor
            print(f"   ğŸ“‰ Score despuÃ©s de penalizaciÃ³n: {score_raw:.4f}")
        
        # Convertir a porcentaje 0-100% con min-max scaling
        score_pct = self.to_percentage(score_raw)
        
        details = {
            'bm25': round(bm25_normalized, 4),
            'cosine': round(cosine_score_raw, 4),  # Raw para logs
            'cosine_normalized': round(cosine_normalized, 4),  # Normalizado
            'coverage': round(coverage_score, 4),
            'score_base': round(score_base, 4),
            'length_bonus': round(bonus, 4),
            'score_raw': round(score_raw, 4),  # Score bruto [0,1]
            'score_pct': score_pct,  # Porcentaje [0-100]
            'final': round(score_raw, 4),  # Mantener compatibilidad
            'weights': self.weights,
            'keywords_found': list(
                self.expand_keywords(answer_keywords) & 
                self.expand_keywords(chunk_keywords)
            )[:5],
            # NUEVO: Info de contradicciÃ³n para debugging
            'contradiction_detected': is_contradiction,
            'contradiction_reason': contradiction_reason if is_contradiction else None,
            'contradiction_penalty': penalty_factor if is_contradiction else 1.0
        }
        
        return score_raw, details
    
    def detect_ambiguity(self, ranked_chunks):
        if len(ranked_chunks) < 2:
            return {'is_ambiguous': False, 'reason': 'Menos de 2 chunks'}
        
        top1_score = ranked_chunks[0][1]
        top2_score = ranked_chunks[1][1]
        score_diff = top1_score - top2_score
        is_ambiguous = score_diff < 0.08
        
        return {
            'is_ambiguous': is_ambiguous,
            'score_diff': round(score_diff, 4),
            'top1_score': round(top1_score, 4),
            'top2_score': round(top2_score, 4),
            'threshold': 0.08
        }
    
    def validate_answer(self, question: str, user_answer: str, chunks):
        if not chunks or len(chunks) == 0:
            return {
                'is_valid': False,
                'confidence': 0.0,
                'feedback': 'No hay chunks disponibles para validacion',
                'category': 'error'
            }
        
        if len(user_answer.strip()) < 10:
            return {
                'is_valid': False,
                'confidence': 0.0,
                'feedback': 'La respuesta es demasiado corta (minimo 10 caracteres)',
                'category': 'error'
            }
        
        scored_chunks = []
        for chunk in chunks:
            score, details = self.hybrid_score(question, user_answer, chunk, chunks)
            scored_chunks.append((chunk, score, details))
        
        ranked_chunks = sorted(scored_chunks, key=lambda x: x[1], reverse=True)
        top_k = ranked_chunks[:3]
        
        ambiguity = self.detect_ambiguity([(c, s) for c, s, _ in ranked_chunks])
        
        best_chunk, best_score_raw, best_details = top_k[0]
        best_score_pct = best_details['score_pct']  # Usar porcentaje para UI
        
        # ClasificaciÃ³n basada en score_raw (0-1)
        if best_score_raw >= self.thresholds['excelente']:
            category = 'excelente'
            is_valid = True
            feedback = 'Excelente! Tu respuesta captura perfectamente el contenido.'
        elif best_score_raw >= self.thresholds['bueno']:
            category = 'bueno'
            is_valid = True
            feedback = 'Muy bien. Tu respuesta es correcta y bien fundamentada.'
        elif best_score_raw >= self.thresholds['aceptable']:
            category = 'aceptable'
            is_valid = True
            feedback = 'Aceptable. Tu respuesta esta en la direccion correcta.'
        else:
            category = 'necesita_mejorar'
            is_valid = False
            feedback = 'Tu respuesta necesita mÃ¡s trabajo. Revisa el material.'
        
        result = {
            'is_valid': is_valid,
            'confidence': best_score_pct,  # Porcentaje 0-100
            'score_raw': round(best_score_raw, 4),  # Score bruto [0,1]
            'feedback': feedback,
            'category': category,
            'best_chunk': {
                'text': best_chunk['text_full'][:200] + '...' if len(best_chunk['text_full']) > 200 else best_chunk['text_full'],
                'page': best_chunk.get('page_number', 'N/A'),
                'chunk_id': best_chunk.get('chunk_id', 'N/A')
            },
            'top_3_scores': [
                {
                    'score': d['score_pct'],  # Porcentaje
                    'score_raw': d['score_raw'],  # Bruto
                    'chunk_id': c.get('chunk_id', 'N/A'),
                    'details': d
                }
                for c, s, d in top_k
            ],
            'ambiguity': ambiguity,
            'thresholds': {k: v * 100 for k, v in self.thresholds.items()},
            'scoring_method': 'HybridValidator (BM25 + Cosine + Coverage)',
            'weights_used': self.weights
        }
        
        return result
