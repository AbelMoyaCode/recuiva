"""
ANALIZADOR DE CONTENIDO INTELIGENTE - RECUIVA
==============================================

Sistema de an√°lisis ling√º√≠stico profundo SIN usar LLMs externos.
Basado en patrones ling√º√≠sticos, morfolog√≠a y sintaxis del espa√±ol.

Autor: Abel Jes√∫s Moya Acosta
Fecha: 10 de noviembre de 2025
Proyecto: Recuiva - Active Recall con IA

CARACTER√çSTICAS:
- An√°lisis morfol√≥gico (sustantivos, verbos, adjetivos)
- Detecci√≥n de patrones sint√°cticos
- Clasificaci√≥n sem√°ntica basada en reglas
- Compatible con UTF-8, emojis, caracteres especiales
- Robusto ante im√°genes/tablas en PDFs
- UNIVERSAL: Funciona para literatura, ciencia, t√©cnico
"""

import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from collections import Counter

# Importar indicadores universales
from universal_entity_types import (
    UNIVERSAL_INDICATORS,
    FUNCTION_WORDS,
    EntityType,
    is_function_word,
    get_all_indicators
)

@dataclass
class ChunkAnalysis:
    """Resultado del an√°lisis profundo de un chunk"""
    content_type: str  # narrative, academic, technical, descriptive, procedural
    confidence: float  # 0.0 a 1.0
    key_entities: List[str]  # Nombres propios, conceptos clave
    main_verbs: List[str]  # Verbos principales
    patterns_detected: List[str]  # Patrones encontrados
    linguistic_features: Dict[str, any]  # Caracter√≠sticas ling√º√≠sticas
    justification: str  # Explicaci√≥n de la decisi√≥n


class ContentAnalyzer:
    """
    Motor de an√°lisis ling√º√≠stico que procesa chunks de texto
    y detecta su tipo, conceptos clave y estructura
    """
    
    # ========================================
    # PATRONES LING√ú√çSTICOS UNIVERSALES
    # ========================================
    
    DEFINITION_PATTERNS = [
        r'(?:se define|se entiende|consiste en|significa|es el|es la|es un|es una)\s+',
        r'(?:llamado|conocido como|denominado)\s+',
        r'(?:concepto de|noci√≥n de|idea de)\s+'
    ]
    
    EXAMPLE_PATTERNS = [
        r'(?:por ejemplo|como|tal como|a saber|verbigracia)',
        r'(?:tales como|entre ellos|incluyendo)',
        r'(?:caso de|ejemplo de)\s+'
    ]
    
    COMPARISON_PATTERNS = [
        r'(?:mientras que|sin embargo|no obstante|por el contrario)',
        r'(?:a diferencia de|en contraste con|comparado con)',
        r'(?:similar a|parecido a|an√°logo a)\s+'
    ]
    
    CAUSALITY_PATTERNS = [
        r'(?:porque|debido a|dado que|puesto que|ya que)',
        r'(?:por lo tanto|por consiguiente|en consecuencia)',
        r'(?:causa de|raz√≥n de|motivo de)\s+'
    ]
    
    PROCEDURAL_PATTERNS = [
        r'(?:primero|segundo|tercero|finalmente|por √∫ltimo)',
        r'(?:paso \d+|etapa \d+|fase \d+)',
        r'(?:se debe|hay que|es necesario)\s+'
    ]
    
    # Verbos narrativos (acciones en historias)
    NARRATIVE_VERBS = [
        'dijo', 'exclam√≥', 'pregunt√≥', 'respondi√≥', 'grit√≥', 'susurr√≥',
        'mir√≥', 'vio', 'observ√≥', 'contempl√≥', 'escuch√≥',
        'camin√≥', 'corri√≥', 'salt√≥', 'entr√≥', 'sali√≥',
        'pens√≥', 'record√≥', 'imagin√≥', 'sinti√≥', 'temi√≥'
    ]
    
    # Verbos acad√©micos (an√°lisis, investigaci√≥n)
    ACADEMIC_VERBS = [
        'analiza', 'investiga', 'demuestra', 'comprueba', 'verifica',
        'establece', 'determina', 'identifica', 'clasifica', 'categoriza',
        'explica', 'describe', 'define', 'conceptualiza', 'teoriza'
    ]
    
    # Verbos t√©cnicos (procedimientos, instrucciones)
    TECHNICAL_VERBS = [
        'configura', 'instala', 'ejecuta', 'compila', 'depura',
        'implementa', 'desarrolla', 'dise√±a', 'construye', 'programa',
        'calcula', 'procesa', 'optimiza', 'eval√∫a', 'mide'
    ]
    
    # Marcadores de metadata (NO son contenido √∫til)
    METADATA_MARKERS = [
        r'LIBRO DESCARGADO',
        r'WWW\.[A-Z]+\.(COM|ORG|NET)',
        r'FUENTE:.*PROJECT',
        r'PUBLICADO.*\d{4}',
        r'ISBN.*\d',
        r'COPYRIGHT|¬©|\(C\)',
        r'DOMINIO P[U√ö]BLICO',
        r'^\*+\s*$',  # L√≠neas de asteriscos
        r'^[\-\_]{5,}$',  # L√≠neas decorativas
        r'^\d+\s*$'  # Solo n√∫meros (paginaci√≥n)
    ]
    
    def __init__(self):
        """Inicializa el analizador"""
        self.stats = {
            'chunks_analyzed': 0,
            'types_detected': Counter(),
            'patterns_found': Counter()
        }
    
    def analyze(self, chunk: str) -> ChunkAnalysis:
        """
        Analiza un chunk de texto en profundidad
        
        Args:
            chunk: Texto a analizar
            
        Returns:
            ChunkAnalysis: Resultado del an√°lisis completo
        """
        self.stats['chunks_analyzed'] += 1
        
        # Limpiar chunk
        clean_chunk = self._clean_chunk(chunk)
        
        # Detectar si es metadata (no contenido)
        if self._is_metadata(clean_chunk):
            return self._create_metadata_analysis(chunk)
        
        # An√°lisis morfol√≥gico
        entities = self._extract_entities(clean_chunk)
        verbs = self._extract_verbs(clean_chunk)
        
        # An√°lisis de patrones
        patterns = self._detect_patterns(clean_chunk)
        
        # Caracter√≠sticas ling√º√≠sticas
        features = self._extract_linguistic_features(clean_chunk)
        
        # Clasificaci√≥n sem√°ntica
        content_type, confidence, justification = self._classify_content(
            clean_chunk, entities, verbs, patterns, features
        )
        
        self.stats['types_detected'][content_type] += 1
        for pattern in patterns:
            self.stats['patterns_found'][pattern] += 1
        
        return ChunkAnalysis(
            content_type=content_type,
            confidence=confidence,
            key_entities=entities,
            main_verbs=verbs,
            patterns_detected=patterns,
            linguistic_features=features,
            justification=justification
        )
    
    def _clean_chunk(self, text: str) -> str:
        """Limpia el chunk preservando estructura"""
        # Normalizar espacios pero preservar saltos de l√≠nea significativos
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()
    
    def _is_metadata(self, text: str) -> bool:
        """Detecta si el chunk es metadata (no contenido √∫til)"""
        # Muy corto (< 50 caracteres)
        if len(text) < 50:
            return True
        
        # Coincide con patrones de metadata
        for pattern in self.METADATA_MARKERS:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        # M√°s del 40% es n√∫meros/s√≠mbolos (probablemente tabla/f√≥rmula sin contexto)
        non_alpha = len(re.findall(r'[^a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë\s]', text))
        if non_alpha / len(text) > 0.4:
            return True
        
        return False
    
    def _create_metadata_analysis(self, chunk: str) -> ChunkAnalysis:
        """Crea an√°lisis para chunks de metadata"""
        return ChunkAnalysis(
            content_type='metadata',
            confidence=1.0,
            key_entities=[],
            main_verbs=[],
            patterns_detected=['metadata_detected'],
            linguistic_features={'is_metadata': True},
            justification="Chunk identificado como metadata (informaci√≥n de publicaci√≥n, copyright, etc.) y excluido de generaci√≥n de preguntas"
        )
    
    def _extract_entities(self, text: str) -> List[str]:
        """
        MEJORADO: Extrae entidades usando INDICADORES UNIVERSALES
        
        ANTES: Solo nombres con t√≠tulos de nobleza (conde, rey, cardenal)
        AHORA: Entidades de CUALQUIER dominio:
        ‚úÖ Literatura: "se√±or Dreux", "reina Mar√≠a Antonieta"
        ‚úÖ Ciencia: "prote√≠na BRCA1", "enzima catalasa", "compuesto X"
        ‚úÖ T√©cnico: "algoritmo QuickSort", "m√©todo Agile"
        ‚úÖ Acad√©mico: "teor√≠a de la relatividad", "ley de Newton"
        """
        entities = []
        
        # PASO 1: EXTRACCI√ìN UNIVERSAL CON INDICADORES  
        # Buscar entidades usando indicadores de TODOS los tipos
        
        # DEBUG LOG
        print(f"\nüî¨ _extract_entities: Analizando {len(text)} caracteres")
        print(f"   Primeras 150 chars: {text[:150]}...")
        
        # Stopwords que NO deben aparecer inmediatamente despu√©s del indicador
        INDICATOR_STOPWORDS = {
            'se', 'que', 'con', 'sin', 'por', 'para', 'como', 'pero',
            'cuando', 'donde', 'quien', 'cual', 'es', 'son', 'fue', 'fueron'
        }
        
        for entity_type, indicators in UNIVERSAL_INDICATORS.items():
            for indicator in indicators:
                # Patr√≥n SIMPLIFICADO: indicador + 1-3 PALABRAS (no oraciones completas)
                # Ejemplos correctos: 
                #   "m√©todo inductivo" ‚Üí inductivo ‚úÖ
                #   "teor√≠a de la relatividad" ‚Üí relatividad ‚úÖ
                #   "prote√≠na BRCA1" ‚Üí BRCA1 ‚úÖ
                #   "algoritmo QuickSort" ‚Üí QuickSort ‚úÖ
                
                # Captura: palabra1 [de/del palabra2] [palabra3]
                pattern = rf'\b(?:el|la|los|las|un|una)?\s*{re.escape(indicator)}\s+([A-Z√Å√â√ç√ì√ö√ë][\w√°√©√≠√≥√∫√±]+(?:\s+(?:de|del)\s+[A-Z√Å√â√ç√ì√ö√ë][\w√°√©√≠√≥√∫√±]+)?(?:\s+[A-Z√Å√â√ç√ì√ö√ë][\w√°√©√≠√≥√∫√±]+)?)\b'
                
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    entity_text = match.group(1).strip()
                    
                    # FILTRO 1: Truncar en stopwords (eliminar todo despu√©s de stopword)
                    words = entity_text.split()
                    clean_words = []
                    for word in words:
                        if word.lower() in INDICATOR_STOPWORDS:
                            break  # Detener en stopword
                        clean_words.append(word)
                    
                    if not clean_words:
                        continue
                    
                    entity_text = ' '.join(clean_words)
                    first_word = entity_text.split()[0].lower()
                    
                    # FILTRO 2: NO debe ser palabra funcional
                    if not is_function_word(first_word):
                        # Limpiar conectores finales
                        entity_text = re.sub(r'\s+(?:de|del|y|con)\s*$', '', entity_text)
                        
                        # Agregar si tiene longitud razonable y no est√° duplicado
                        if len(entity_text) >= 3 and entity_text not in entities:
                            entities.append(entity_text)
        
        # PASO 2: NOMBRES PROPIOS SIN INDICADOR (para casos como "Mar√≠a Antonieta dijo...")
        # Blacklist UNIVERSAL (no espec√≠fica de dominio)
        UNIVERSAL_BLACKLIST = FUNCTION_WORDS | {
            'Como', 'Pero', 'Cuando', 'Cierto', 'Verdad', 'Inmediatamente',
            'Esper', 'Cabal', 'Loro', 'Eux', 'Mot', 'Te', 'Hasta'
        }
        
        # Patr√≥n: 2-4 palabras capitalizadas con alta confianza
        name_pattern = r'\b([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]{2,}(?:\s+(?:de|del|y)\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]{2,})*(?:\s+[A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]{2,}){1,2})\b'
        
        for match in re.finditer(name_pattern, text):
            name_candidate = match.group(1).strip()
            words = name_candidate.split()
            
            # VALIDACI√ìN 1: M√≠nimo 2 palabras
            if len(words) < 2:
                continue
            
            # VALIDACI√ìN 2: Cada palabra m√≠nimo 3 caracteres
            if any(len(w) < 3 for w in words if w.lower() not in {'de', 'del', 'y'}):
                continue
            
            # VALIDACI√ìN 3: Primera palabra NO en blacklist
            if words[0] in UNIVERSAL_BLACKLIST or words[0].lower() in UNIVERSAL_BLACKLIST:
                continue
            
            # VALIDACI√ìN 4: NO contener blacklist intermedias
            if any(w in UNIVERSAL_BLACKLIST or w.lower() in UNIVERSAL_BLACKLIST for w in words):
                continue
            
            # VALIDACI√ìN 5: Contexto verbal (cerca de verbo de persona)
            start = max(0, match.start() - 50)
            end = min(len(text), match.end() + 50)
            context = text[start:end].lower()
            
            person_verbs = {
                'dijo', 'pregunt√≥', 'respondi√≥', 'pens√≥', 'mir√≥', 'vio',
                'entr√≥', 'sali√≥', 'camin√≥', 'hizo', 'fue', 'era', 'estaba',
                'ten√≠a', 'hab√≠a', 'luc√≠a', 'llevaba', 'ofrec√≠a', 'crey√≥'
            }
            
            has_verb_context = any(verb in context for verb in person_verbs)
            has_person_article = re.search(r'\b(?:el|la)\s+' + re.escape(words[0].lower()), context)
            
            if has_verb_context or has_person_article:
                name_clean = re.sub(r'\s+(?:de|del|y)\s*$', '', name_candidate)
                if name_clean and name_clean not in entities:
                    entities.append(name_clean)
        
        # PASO 3: T√©rminos entre comillas (SOLO si son conceptos t√©cnicos, no ejemplos)
        quoted_terms = re.findall(r'["¬´]([^"¬ª]{5,40})["¬ª]', text)
        for term in quoted_terms:
            term_clean = term.strip()
            # FILTRO: Solo agregar si NO es un ejemplo o frase completa
            # Rechazar si tiene verbos conjugados comunes o art√≠culos al inicio
            if len(term_clean.split()) <= 5 and term_clean not in entities:
                # Validar que NO sea una oraci√≥n (no debe tener verbo)
                common_verbs = {'es', 'son', 'est√°', 'est√°n', 'fue', 'fueron', 'era', 'eran', 'tiene', 'tienen', 'hay'}
                if not any(verb in term_clean.lower().split() for verb in common_verbs):
                    entities.append(term_clean)
        
        # PASO 4: Deduplicar preservando orden
        seen = set()
        unique_entities = []
        for entity in entities:
            entity_lower = entity.lower().strip()
            if entity_lower not in seen and len(entity) > 3:
                seen.add(entity_lower)
                unique_entities.append(entity.strip())
        
        # DEBUG LOG FINAL
        print(f"   ‚úÖ Entidades extra√≠das: {unique_entities[:5]}")  # Primeras 5
        print(f"   üìä Total entidades: {len(unique_entities)}")
        
        return unique_entities[:10]  # Top 10 entidades
    
    def _extract_verbs(self, text: str) -> List[str]:
        """Extrae verbos principales del chunk"""
        text_lower = text.lower()
        verbs_found = []
        
        # Buscar verbos narrativos
        for verb in self.NARRATIVE_VERBS:
            if verb in text_lower:
                verbs_found.append(('narrative', verb))
        
        # Buscar verbos acad√©micos
        for verb in self.ACADEMIC_VERBS:
            if verb in text_lower:
                verbs_found.append(('academic', verb))
        
        # Buscar verbos t√©cnicos
        for verb in self.TECHNICAL_VERBS:
            if verb in text_lower:
                verbs_found.append(('technical', verb))
        
        return verbs_found[:8]  # Top 8
    
    def _detect_patterns(self, text: str) -> List[str]:
        """Detecta patrones sint√°cticos en el texto"""
        patterns = []
        
        if any(re.search(p, text, re.IGNORECASE) for p in self.DEFINITION_PATTERNS):
            patterns.append('definition')
        
        if any(re.search(p, text, re.IGNORECASE) for p in self.EXAMPLE_PATTERNS):
            patterns.append('example')
        
        if any(re.search(p, text, re.IGNORECASE) for p in self.COMPARISON_PATTERNS):
            patterns.append('comparison')
        
        if any(re.search(p, text, re.IGNORECASE) for p in self.CAUSALITY_PATTERNS):
            patterns.append('causality')
        
        if any(re.search(p, text, re.IGNORECASE) for p in self.PROCEDURAL_PATTERNS):
            patterns.append('procedural')
        
        # Detectar di√°logos
        if '"' in text or '¬´' in text or '‚Äî' in text:
            patterns.append('dialogue')
        
        # Detectar listas/enumeraciones
        if re.search(r'(?:\n|^)\s*[\-\*\‚Ä¢]\s+', text):
            patterns.append('list')
        
        # Detectar n√∫meros/datos
        if re.search(r'\b\d{4}\b|\d+%|\d+\s*(?:metros|a√±os|personas|km)', text):
            patterns.append('numerical_data')
        
        # Detectar f√≥rmulas matem√°ticas
        if re.search(r'[=].*[\d+\-*/]|‚à´|‚àë|‚àö|‚àè|‚àÜ', text):
            patterns.append('mathematical_formula')
        
        return patterns
    
    def _extract_linguistic_features(self, text: str) -> Dict[str, any]:
        """Extrae caracter√≠sticas ling√º√≠sticas del chunk"""
        words = text.split()
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]
        
        return {
            'word_count': len(words),
            'sentence_count': len(sentences),
            'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0,
            'avg_sentence_length': len(words) / len(sentences) if sentences else 0,
            'has_questions': '¬ø' in text or '?' in text,
            'has_exclamations': '¬°' in text or '!' in text,
            'capital_ratio': sum(1 for c in text if c.isupper()) / len(text) if text else 0,
            'punctuation_density': len(re.findall(r'[.,;:!?]', text)) / len(words) if words else 0
        }
    
    def _classify_content(
        self,
        text: str,
        entities: List[str],
        verbs: List[Tuple[str, str]],
        patterns: List[str],
        features: Dict
    ) -> Tuple[str, float, str]:
        """
        Clasifica el tipo de contenido y genera justificaci√≥n
        
        Returns:
            (tipo, confianza, justificaci√≥n)
        """
        scores = {
            'narrative': 0.0,
            'academic': 0.0,
            'technical': 0.0,
            'descriptive': 0.0,
            'procedural': 0.0
        }
        
        reasons = []
        
        # AN√ÅLISIS 1: Verbos
        narrative_verbs = sum(1 for t, _ in verbs if t == 'narrative')
        academic_verbs = sum(1 for t, _ in verbs if t == 'academic')
        technical_verbs = sum(1 for t, _ in verbs if t == 'technical')
        
        if narrative_verbs >= 2:
            scores['narrative'] += 0.3
            reasons.append(f"Contiene {narrative_verbs} verbos narrativos ({', '.join([v for t,v in verbs if t=='narrative'][:3])})")
        
        if academic_verbs >= 2:
            scores['academic'] += 0.3
            reasons.append(f"Contiene {academic_verbs} verbos acad√©micos")
        
        if technical_verbs >= 2:
            scores['technical'] += 0.3
            reasons.append(f"Contiene {technical_verbs} verbos t√©cnicos")
        
        # AN√ÅLISIS 2: Patrones
        if 'dialogue' in patterns:
            scores['narrative'] += 0.25
            reasons.append("Detectado di√°logo directo (comillas)")
        
        if 'definition' in patterns:
            scores['academic'] += 0.2
            reasons.append("Detectado patr√≥n de definici√≥n")
        
        if 'procedural' in patterns or 'list' in patterns:
            scores['procedural'] += 0.3
            reasons.append("Detectado patr√≥n procedimental o lista")
        
        if 'mathematical_formula' in patterns:
            scores['technical'] += 0.25
            reasons.append("Detectada f√≥rmula matem√°tica")
        
        if 'numerical_data' in patterns:
            scores['technical'] += 0.15
            reasons.append("Contiene datos num√©ricos")
        
        # AN√ÅLISIS 3: Entidades
        if len(entities) >= 3:
            # Muchas entidades ‚Üí probablemente narrativa o descriptiva
            if narrative_verbs > 0:
                scores['narrative'] += 0.2
            else:
                scores['descriptive'] += 0.2
            reasons.append(f"Detectadas {len(entities)} entidades clave")
        
        # AN√ÅLISIS 4: Caracter√≠sticas ling√º√≠sticas
        if features['has_questions']:
            scores['academic'] += 0.1
        
        if features['avg_sentence_length'] > 20:
            scores['academic'] += 0.15
            reasons.append("Oraciones largas (estilo acad√©mico)")
        
        # Determinar tipo ganador
        winner_type = max(scores, key=scores.get)
        confidence = scores[winner_type]
        
        # Si no hay puntuaci√≥n clara, es descriptivo
        if confidence < 0.3:
            winner_type = 'descriptive'
            confidence = 0.5
            reasons.append("Contenido descriptivo general (sin patrones claros)")
        
        # Normalizar confianza a [0, 1]
        confidence = min(1.0, confidence)
        
        # Generar justificaci√≥n
        justification = f"Clasificado como '{winner_type}' con confianza {confidence:.0%}. Razones: " + "; ".join(reasons)
        
        return winner_type, confidence, justification
    
    def get_stats(self) -> Dict:
        """Retorna estad√≠sticas del analizador"""
        return {
            'chunks_analyzed': self.stats['chunks_analyzed'],
            'types_distribution': dict(self.stats['types_detected']),
            'patterns_frequency': dict(self.stats['patterns_found'].most_common(10))
        }
