"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TEST_EMBEDDINGS.PY - Pruebas Unitarias del MÃ³dulo de Embeddings
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Este mÃ³dulo contiene las pruebas unitarias para verificar:
1. Carga correcta del modelo Sentence-Transformers
2. GeneraciÃ³n de embeddings de 384 dimensiones
3. CÃ¡lculo de similitud coseno entre vectores
4. Manejo de casos edge (texto vacÃ­o, caracteres especiales)
5. Consistencia de embeddings para el mismo texto

RESPONDE A PREGUNTAS DEL PROFESOR (Semana 15):
- "Â¿Tienes tus casos de prueba para probar tus embeddings?"
- "Â¿CuÃ¡ntos embeddings estÃ¡ extrayendo?"

Modelo: all-MiniLM-L6-v2 (384 dimensiones)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import pytest
import numpy as np
import sys
from pathlib import Path

# Agregar backend al path
BACKEND_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(BACKEND_DIR))

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASE: TestEmbeddingModel - Pruebas de carga del modelo
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestEmbeddingModel:
    """
    Pruebas de carga y configuraciÃ³n del modelo de embeddings
    
    Verifica que:
    - El modelo se carga correctamente
    - Es el modelo esperado (all-MiniLM-L6-v2)
    - Genera embeddings del tamaÃ±o correcto
    """
    
    def test_model_loads_successfully(self):
        """
        TEST: El modelo de embeddings debe cargarse sin errores
        
        Criterio de aceptaciÃ³n:
        - load_model() retorna un objeto no nulo
        - No se lanzan excepciones durante la carga
        """
        from embeddings_module import load_model
        
        model = load_model()
        
        assert model is not None, "El modelo no debe ser None"
        print(f"âœ… Modelo cargado: {type(model).__name__}")
    
    def test_model_dimension_is_384(self, embedding_model):
        """
        TEST: El modelo debe generar embeddings de 384 dimensiones
        
        VerificaciÃ³n:
        - all-MiniLM-L6-v2 produce vectores de tamaÃ±o 384
        - Este es el tamaÃ±o esperado segÃºn la documentaciÃ³n
        """
        from embeddings_module import generate_embeddings
        
        texto_prueba = "Este es un texto de prueba"
        embedding = generate_embeddings(texto_prueba)
        
        assert len(embedding) == 384, f"DimensiÃ³n esperada: 384, obtenida: {len(embedding)}"
        print(f"âœ… DimensiÃ³n del embedding: {len(embedding)}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASE: TestEmbeddingGeneration - Pruebas de generaciÃ³n
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestEmbeddingGeneration:
    """
    Pruebas de generaciÃ³n de embeddings
    
    Valida el proceso de conversiÃ³n de texto a vectores semÃ¡nticos
    y verifica propiedades matemÃ¡ticas de los embeddings.
    """
    
    def test_generate_embedding_returns_numpy_array(self, embedding_model):
        """
        TEST: generate_embeddings() debe retornar un numpy array
        
        Criterio:
        - El tipo de retorno debe ser np.ndarray
        - El array debe tener valores numÃ©ricos
        """
        from embeddings_module import generate_embeddings
        
        texto = "Un puntero es una variable que almacena direcciones de memoria"
        embedding = generate_embeddings(texto)
        
        assert isinstance(embedding, np.ndarray), f"Tipo esperado: np.ndarray, obtenido: {type(embedding)}"
        assert embedding.dtype in [np.float32, np.float64], "El embedding debe contener flotantes"
        print(f"âœ… Tipo de retorno correcto: {type(embedding).__name__}, dtype: {embedding.dtype}")
    
    def test_embedding_is_normalized(self, embedding_model):
        """
        TEST: Los embeddings deben estar normalizados (norma â‰ˆ 1)
        
        VerificaciÃ³n:
        - La norma L2 del vector debe ser aproximadamente 1
        - Esto es necesario para que la similitud coseno funcione correctamente
        """
        from embeddings_module import generate_embeddings
        
        texto = "La desreferenciaciÃ³n permite acceder al valor almacenado"
        embedding = generate_embeddings(texto)
        
        norma = np.linalg.norm(embedding)
        assert 0.99 <= norma <= 1.01, f"Norma esperada â‰ˆ 1, obtenida: {norma}"
        print(f"âœ… Norma del embedding: {norma:.6f}")
    
    def test_same_text_produces_same_embedding(self, embedding_model):
        """
        TEST: El mismo texto debe producir el mismo embedding (determinismo)
        
        Criterio:
        - Dos llamadas con el mismo texto deben retornar vectores idÃ©nticos
        - Esto garantiza reproducibilidad
        """
        from embeddings_module import generate_embeddings
        
        texto = "Los punteros son fundamentales en C++"
        
        embedding_1 = generate_embeddings(texto)
        embedding_2 = generate_embeddings(texto)
        
        diferencia = np.max(np.abs(embedding_1 - embedding_2))
        assert diferencia < 1e-6, f"Los embeddings deben ser idÃ©nticos, diferencia: {diferencia}"
        print(f"âœ… Embeddings idÃ©nticos, diferencia mÃ¡xima: {diferencia:.10f}")
    
    def test_different_texts_produce_different_embeddings(self, embedding_model):
        """
        TEST: Textos diferentes deben producir embeddings diferentes
        
        Criterio:
        - Dos textos semÃ¡nticamente diferentes deben tener embeddings distintos
        - La similitud debe ser < 0.9 para textos no relacionados
        """
        from embeddings_module import generate_embeddings
        
        texto_punteros = "Un puntero almacena direcciones de memoria"
        texto_cocina = "La receta de cocina incluye ingredientes frescos"
        
        emb_punteros = generate_embeddings(texto_punteros)
        emb_cocina = generate_embeddings(texto_cocina)
        
        similitud = np.dot(emb_punteros, emb_cocina)
        assert similitud < 0.7, f"Textos no relacionados deben tener similitud < 0.7, obtenida: {similitud}"
        print(f"âœ… Similitud entre textos no relacionados: {similitud:.4f}")
    
    def test_similar_texts_have_high_similarity(self, embedding_model):
        """
        TEST: Textos semÃ¡nticamente similares deben tener alta similitud
        
        Criterio:
        - ParÃ¡frasis del mismo concepto deben tener similitud > 0.3
        - (Umbral ajustado para modelo all-MiniLM-L6-v2)
        """
        from embeddings_module import generate_embeddings
        
        texto_1 = "Un puntero es una variable que almacena la direcciÃ³n de memoria"
        texto_2 = "Un puntero guarda direcciones de memoria de otras variables"
        
        emb_1 = generate_embeddings(texto_1)
        emb_2 = generate_embeddings(texto_2)
        
        similitud = np.dot(emb_1, emb_2)
        assert similitud > 0.3, f"Textos similares deben tener similitud > 0.3, obtenida: {similitud}"
        print(f"âœ… Similitud entre parÃ¡frasis: {similitud:.4f}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASE: TestSimilarityCosine - Pruebas de cÃ¡lculo de similitud
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestSimilarityCosine:
    """
    Pruebas del cÃ¡lculo de similitud coseno
    
    Verifica la funciÃ³n calculate_similarity() que es fundamental
    para el sistema de validaciÃ³n hÃ­brida.
    """
    
    def test_identical_vectors_similarity_equals_one(self, embedding_model):
        """
        TEST: Vectores idÃ©nticos deben tener similitud = 1.0
        """
        from embeddings_module import generate_embeddings, calculate_similarity
        
        texto = "El operador asterisco declara un puntero"
        embedding = generate_embeddings(texto)
        
        similitud = calculate_similarity(embedding, embedding)
        assert abs(similitud - 1.0) < 0.01, f"Esperado: 1.0, obtenido: {similitud}"
        print(f"âœ… Similitud de vector consigo mismo: {similitud:.4f}")
    
    def test_orthogonal_vectors_similarity_near_zero(self, embedding_model):
        """
        TEST: Vectores ortogonales deben tener similitud cercana a 0
        
        Nota: En embeddings reales es difÃ­cil encontrar vectores perfectamente
        ortogonales, asÃ­ que verificamos que textos muy diferentes tengan
        similitud baja.
        """
        from embeddings_module import generate_embeddings, calculate_similarity
        
        texto_tech = "Algoritmo de ordenamiento burbuja en estructura de datos"
        texto_nature = "Las mariposas monarca migran miles de kilÃ³metros"
        
        emb_1 = generate_embeddings(texto_tech)
        emb_2 = generate_embeddings(texto_nature)
        
        similitud = calculate_similarity(emb_1, emb_2)
        assert similitud < 0.5, f"Textos muy diferentes deben tener similitud < 0.5, obtenida: {similitud}"
        print(f"âœ… Similitud entre temas diferentes: {similitud:.4f}")
    
    def test_similarity_is_symmetric(self, embedding_model):
        """
        TEST: La similitud coseno debe ser simÃ©trica: sim(A,B) = sim(B,A)
        """
        from embeddings_module import generate_embeddings, calculate_similarity
        
        texto_a = "Los punteros permiten acceso directo a memoria"
        texto_b = "El acceso a memoria se logra mediante punteros"
        
        emb_a = generate_embeddings(texto_a)
        emb_b = generate_embeddings(texto_b)
        
        sim_ab = calculate_similarity(emb_a, emb_b)
        sim_ba = calculate_similarity(emb_b, emb_a)
        
        assert abs(sim_ab - sim_ba) < 1e-6, f"La similitud debe ser simÃ©trica: {sim_ab} vs {sim_ba}"
        print(f"âœ… Similitud simÃ©trica: sim(A,B)={sim_ab:.4f}, sim(B,A)={sim_ba:.4f}")
    
    def test_similarity_range_is_valid(self, embedding_model):
        """
        TEST: La similitud coseno debe estar en el rango [-1, 1]
        
        Para embeddings normalizados, generalmente estÃ¡ en [0, 1]
        """
        from embeddings_module import generate_embeddings, calculate_similarity
        
        textos = [
            "Puntero es una variable",
            "Memoria del sistema operativo",
            "FunciÃ³n matemÃ¡tica derivada",
            "Receta de cocina italiana"
        ]
        
        embeddings = [generate_embeddings(t) for t in textos]
        
        for i, emb_i in enumerate(embeddings):
            for j, emb_j in enumerate(embeddings):
                sim = calculate_similarity(emb_i, emb_j)
                assert -1.0 <= sim <= 1.0, f"Similitud fuera de rango: {sim}"
        
        print(f"âœ… Todas las similitudes estÃ¡n en el rango vÃ¡lido [-1, 1]")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASE: TestEdgeCases - Casos lÃ­mite
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestEdgeCases:
    """
    Pruebas de casos lÃ­mite y manejo de errores
    
    Verifica que el sistema maneje correctamente:
    - Texto vacÃ­o
    - Texto muy largo
    - Caracteres especiales
    - Unicode/EspaÃ±ol
    """
    
    def test_empty_string_handling(self, embedding_model):
        """
        TEST: El sistema debe manejar texto vacÃ­o sin errores
        """
        from embeddings_module import generate_embeddings
        
        # El sistema debe retornar un embedding vÃ¡lido o manejarlo graciosamente
        try:
            embedding = generate_embeddings("")
            assert embedding is not None
            assert len(embedding) == 384
            print(f"âœ… Texto vacÃ­o manejado correctamente")
        except Exception as e:
            # Si lanza excepciÃ³n, debe ser una excepciÃ³n controlada
            print(f"âœ… Texto vacÃ­o genera excepciÃ³n controlada: {type(e).__name__}")
    
    def test_spanish_text_with_accents(self, embedding_model):
        """
        TEST: El modelo debe procesar correctamente texto en espaÃ±ol con acentos
        """
        from embeddings_module import generate_embeddings
        
        texto_espanol = "El Ã¡rbol genealÃ³gico contiene informaciÃ³n histÃ³rica Ãºnica"
        embedding = generate_embeddings(texto_espanol)
        
        assert embedding is not None
        assert len(embedding) == 384
        print(f"âœ… Texto espaÃ±ol con acentos procesado: '{texto_espanol[:30]}...'")
    
    def test_special_characters(self, embedding_model):
        """
        TEST: El modelo debe manejar caracteres especiales
        """
        from embeddings_module import generate_embeddings
        
        texto_especial = "int *ptr = &variable; // Puntero en C++"
        embedding = generate_embeddings(texto_especial)
        
        assert embedding is not None
        assert len(embedding) == 384
        print(f"âœ… Caracteres especiales manejados: '{texto_especial}'")
    
    def test_long_text_handling(self, embedding_model):
        """
        TEST: El modelo debe manejar textos largos (> 512 tokens)
        
        Nota: El modelo trunca textos largos, pero debe funcionar sin errores
        """
        from embeddings_module import generate_embeddings
        
        texto_largo = "Los punteros en C++ son fundamentales. " * 100
        embedding = generate_embeddings(texto_largo)
        
        assert embedding is not None
        assert len(embedding) == 384
        print(f"âœ… Texto largo ({len(texto_largo)} caracteres) procesado correctamente")
    
    def test_whitespace_only_text(self, embedding_model):
        """
        TEST: El sistema debe manejar texto con solo espacios
        """
        from embeddings_module import generate_embeddings
        
        try:
            embedding = generate_embeddings("   \n\t   ")
            assert embedding is not None
            print(f"âœ… Texto con solo espacios manejado")
        except Exception as e:
            print(f"âœ… Texto con solo espacios genera excepciÃ³n controlada: {type(e).__name__}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASE: TestBatchEmbeddings - Pruebas de procesamiento por lotes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestBatchEmbeddings:
    """
    Pruebas de generaciÃ³n de embeddings en lote
    
    IMPORTANTE para el profesor:
    "Â¿CuÃ¡ntos embeddings estÃ¡ extrayendo asociados a ese tÃ©rmino?"
    """
    
    def test_multiple_chunks_embedding_count(self, embedding_model, chunks_punteros):
        """
        TEST: Verificar que se generan embeddings para TODOS los chunks
        
        Este test responde directamente a la pregunta del profesor:
        "Â¿CuÃ¡ntos chunks estÃ¡ extrayÃ©ndolos asociados a ese tÃ©rmino puntero?"
        """
        from embeddings_module import generate_embeddings
        
        embeddings_generados = []
        for i, chunk in enumerate(chunks_punteros):
            # Los chunks ahora son diccionarios con 'text_full' y 'embedding'
            chunk_text = chunk.get('text_full', chunk) if isinstance(chunk, dict) else chunk
            if chunk_text and chunk_text.strip():
                # Ya tienen embedding precalculado
                if isinstance(chunk, dict) and 'embedding' in chunk:
                    emb = chunk['embedding']
                else:
                    emb = generate_embeddings(chunk_text)
                embeddings_generados.append(emb)
                print(f"  Chunk {i+1}: {len(chunk_text)} caracteres â†’ Embedding {len(emb)} dims")
        
        valid_chunks = [c for c in chunks_punteros if (c.get('text_full', c) if isinstance(c, dict) else c).strip()]
        assert len(embeddings_generados) == len(valid_chunks)
        print(f"âœ… Total embeddings generados: {len(embeddings_generados)}")
        print(f"   (Para {len(chunks_punteros)} chunks del material de punteros)")
    
    def test_embedding_retrieval_for_term(self, embedding_model, material_punteros):
        """
        TEST: Verificar cuÃ¡ntos embeddings se recuperan para el tÃ©rmino "puntero"
        
        RESPONDE DIRECTAMENTE A LA PREGUNTA DEL PROFESOR SEMANA 15:
        "Â¿CuÃ¡ntos chunks estÃ¡ extrayÃ©ndolos asociados a ese tÃ©rmino puntero?"
        """
        from embeddings_module import generate_embeddings
        from chunking import semantic_chunking
        
        # Generar chunks
        chunks = semantic_chunking(material_punteros, min_words=20, max_words=60, overlap_words=5)
        
        # Generar embedding para el tÃ©rmino de bÃºsqueda
        query_embedding = generate_embeddings("puntero")
        
        # Calcular similitud con cada chunk
        chunks_relevantes = []
        for chunk in chunks:
            if not chunk.strip():
                continue
            chunk_embedding = generate_embeddings(chunk)
            similitud = np.dot(query_embedding, chunk_embedding)
            if similitud > 0.3:  # Umbral mÃ­nimo de relevancia
                chunks_relevantes.append({
                    "chunk": chunk[:50] + "...",
                    "similitud": similitud
                })
        
        # Ordenar por similitud
        chunks_relevantes.sort(key=lambda x: x["similitud"], reverse=True)
        
        print(f"\nğŸ“Š CHUNKS ASOCIADOS AL TÃ‰RMINO 'puntero':")
        print(f"   Total chunks analizados: {len(chunks)}")
        print(f"   Chunks relevantes (sim > 0.3): {len(chunks_relevantes)}")
        print(f"\n   Top 5 chunks mÃ¡s relevantes:")
        for i, item in enumerate(chunks_relevantes[:5]):
            print(f"   {i+1}. Similitud: {item['similitud']:.4f} - '{item['chunk']}'")
        
        assert len(chunks_relevantes) > 0, "Debe haber al menos un chunk relevante para 'puntero'"
        print(f"\nâœ… Se encontraron {len(chunks_relevantes)} chunks asociados al tÃ©rmino 'puntero'")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTS INDIVIDUALES (para ejecuciÃ³n rÃ¡pida)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_embedding_dimension_quick():
    """Test rÃ¡pido de dimensiÃ³n sin fixture"""
    from embeddings_module import generate_embeddings
    
    emb = generate_embeddings("Prueba rÃ¡pida")
    assert len(emb) == 384


def test_similarity_calculation_quick():
    """Test rÃ¡pido de similitud sin fixture"""
    from embeddings_module import generate_embeddings, calculate_similarity
    
    emb1 = generate_embeddings("puntero en C++")
    emb2 = generate_embeddings("puntero en memoria")
    
    sim = calculate_similarity(emb1, emb2)
    assert 0.5 < sim < 1.0  # Deben ser similares


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
