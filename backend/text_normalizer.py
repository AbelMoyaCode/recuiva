"""
Normalizador de Texto para Embeddings
======================================

Corrige errores de extracciÃ³n OCR y normaliza texto antes de generar embeddings.
Esto mejora la calidad de la similitud semÃ¡ntica.

Problemas que soluciona:
- Espacios entre sÃ­labas: "fo to sÃ­n te sis" â†’ "fotosÃ­ntesis"
- Guiones de separaciÃ³n: "trans- formaciÃ³n" â†’ "transformaciÃ³n"
- Espacios mÃºltiples: "El   libro" â†’ "El libro"
- Espacios antes de puntuaciÃ³n: "hola ." â†’ "hola."

Autor: Abel JesÃºs Moya Acosta
Fecha: 17 de noviembre de 2025
Proyecto: Recuiva - Active Recall con IA
"""

import re
from typing import List, Union


def normalize_text(text: str) -> str:
    """
    Normaliza texto para mejorar calidad de embeddings
    
    Transformaciones aplicadas:
    1. Remover espacios entre sÃ­labas (OCR error comÃºn)
    2. Remover guiones de separaciÃ³n de lÃ­nea
    3. Normalizar espacios mÃºltiples
    4. Corregir espacios antes de puntuaciÃ³n
    5. Trimear inicio/fin
    
    Args:
        text: Texto original (puede contener errores OCR)
        
    Returns:
        str: Texto normalizado
        
    Ejemplos:
        >>> normalize_text("La fo to sÃ­n te sis es un proceso")
        'La fotosÃ­ntesis es un proceso'
        
        >>> normalize_text("Las plantas trans- forman luz en energÃ­a")
        'Las plantas transforman luz en energÃ­a'
        
        >>> normalize_text("El   libro   tiene    muchos   espacios")
        'El libro tiene muchos espacios'
        
        >>> normalize_text("Hola , Â¿cÃ³mo estÃ¡s ?")
        'Hola, Â¿cÃ³mo estÃ¡s?'
    """
    if not text or not isinstance(text, str):
        return ""
    
    # 1. Remover espacios innecesarios entre sÃ­labas (error OCR comÃºn)
    # Detecta patrones como: "fo to sÃ­n te sis" (espacios entre letras cortas)
    # Estrategia mejorada: Capturar fragmentos de 1-5 letras con espacios
    
    # Primero: fragmentos muy cortos (1-2 letras)
    for _ in range(5):
        text = re.sub(r'\b(\w{1,2})\s+(\w{1,2})\b', r'\1\2', text)
    
    # Segundo: fragmentos medianos (2-4 letras + 3-6 letras)
    # Ejemplo: "pr ecise" â†’ "precise"
    for _ in range(3):
        text = re.sub(r'\b(\w{2,4})\s+(\w{3,6})\b', r'\1\2', text)
    
    # Tercero: caso especÃ­fico de OCR malo - palabra al final de lÃ­nea
    # Ejemplo: "esun punto" â†’ "es un punto"
    text = re.sub(r'(\w)([a-z]{2,})\s+([a-z])', r'\1 \2 \3', text)
    
    # 2. Remover guiones de separaciÃ³n de lÃ­nea (ej: "trans- formaciÃ³n")
    text = re.sub(r'(\w)-\s+(\w)', r'\1\2', text)
    
    # 3. Normalizar espacios mÃºltiples a un solo espacio
    text = re.sub(r'\s{2,}', ' ', text)
    
    # 4. Remover espacios antes de puntuaciÃ³n
    text = re.sub(r'\s+([.,;:!?Â¿Â¡Â»])', r'\1', text)
    
    # 5. Agregar espacio despuÃ©s de puntuaciÃ³n si no existe
    text = re.sub(r'([.,;:!?])([A-Za-zÃ-ÃºÃ‘Ã±])', r'\1 \2', text)
    
    # 6. Trimear y retornar
    return text.strip()


def normalize_text_batch(texts: List[str]) -> List[str]:
    """
    Normaliza mÃºltiples textos en lote
    
    Args:
        texts: Lista de textos a normalizar
        
    Returns:
        List[str]: Textos normalizados
    """
    return [normalize_text(t) for t in texts]


def detect_ocr_errors(text: str) -> dict:
    """
    Detecta posibles errores OCR en el texto (para debugging)
    
    Args:
        text: Texto a analizar
        
    Returns:
        dict: EstadÃ­sticas de errores detectados
    """
    stats = {
        'fragmented_words': 0,      # Palabras fragmentadas
        'hyphen_breaks': 0,          # Separaciones con guiÃ³n
        'multiple_spaces': 0,        # Espacios mÃºltiples
        'punctuation_spacing': 0,    # Espacios antes de puntuaciÃ³n
        'has_errors': False
    }
    
    # Contar fragmentaciones (palabras de 1-2 letras seguidas)
    fragmented = re.findall(r'\b\w{1,2}\s+\w{1,2}\b', text)
    stats['fragmented_words'] = len(fragmented)
    
    # Contar guiones de separaciÃ³n
    hyphen_breaks = re.findall(r'\w-\s+\w', text)
    stats['hyphen_breaks'] = len(hyphen_breaks)
    
    # Contar espacios mÃºltiples
    multiple_spaces = re.findall(r'\s{2,}', text)
    stats['multiple_spaces'] = len(multiple_spaces)
    
    # Contar espacios antes de puntuaciÃ³n
    punctuation_spacing = re.findall(r'\s+[.,;:!?]', text)
    stats['punctuation_spacing'] = len(punctuation_spacing)
    
    # Determinar si hay errores
    stats['has_errors'] = any([
        stats['fragmented_words'] > 2,
        stats['hyphen_breaks'] > 0,
        stats['multiple_spaces'] > 5,
        stats['punctuation_spacing'] > 3
    ])
    
    return stats


# ===== FUNCIONES AUXILIARES =====

def clean_latex_artifacts(text: str) -> str:
    """
    Remueve artefactos de LaTeX/OCR matemÃ¡tico
    
    Args:
        text: Texto con posibles artefactos LaTeX
        
    Returns:
        str: Texto limpio
    """
    # Remover comandos LaTeX comunes
    text = re.sub(r'\\[a-zA-Z]+\{([^}]*)\}', r'\1', text)
    text = re.sub(r'\\[a-zA-Z]+', '', text)
    
    # Remover sÃ­mbolos matemÃ¡ticos aislados
    text = re.sub(r'\$([^$]*)\$', r'\1', text)
    
    return text


def fix_common_ocr_substitutions(text: str) -> str:
    """
    Corrige sustituciones OCR comunes (caracteres confundidos)
    
    Ejemplos:
    - 'l' (L minÃºscula) â†’ '1' (uno)
    - '0' (cero) â†’ 'O' (o mayÃºscula)
    - 'rn' â†’ 'm'
    
    Args:
        text: Texto con posibles sustituciones
        
    Returns:
        str: Texto corregido
    """
    # Lista de sustituciones comunes
    # (Puede expandirse segÃºn el tipo de PDF)
    substitutions = [
        # (patrÃ³n_incorrecto, correcciÃ³n)
        (r'\bl1\b', 'li'),  # Ejemplo: "l1bro" â†’ "libro"
        (r'\brn\b', 'm'),   # Ejemplo: "forrna" â†’ "forma"
    ]
    
    for pattern, replacement in substitutions:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    return text


# ===== TESTING =====

if __name__ == "__main__":
    """Pruebas del normalizador"""
    
    print("=" * 70)
    print("ğŸ§¹ NORMALIZADOR DE TEXTO - TESTS")
    print("=" * 70)
    
    # Test 1: FragmentaciÃ³n de palabras
    test1 = "La fo to sÃ­n te sis es un pro ce so bi o lÃ³ gi co"
    print(f"\nğŸ“ Test 1 - FragmentaciÃ³n:")
    print(f"   Antes: {test1}")
    print(f"   DespuÃ©s: {normalize_text(test1)}")
    
    # Test 2: Guiones de separaciÃ³n
    test2 = "Las plantas trans- forman la luz solar en ener- gÃ­a quÃ­mica"
    print(f"\nğŸ“ Test 2 - Guiones:")
    print(f"   Antes: {test2}")
    print(f"   DespuÃ©s: {normalize_text(test2)}")
    
    # Test 3: Espacios mÃºltiples
    test3 = "El   libro    tiene     muchos    espacios"
    print(f"\nğŸ“ Test 3 - Espacios mÃºltiples:")
    print(f"   Antes: '{test3}'")
    print(f"   DespuÃ©s: '{normalize_text(test3)}'")
    
    # Test 4: PuntuaciÃ³n
    test4 = "Hola , Â¿cÃ³mo estÃ¡s ? Bien ."
    print(f"\nğŸ“ Test 4 - PuntuaciÃ³n:")
    print(f"   Antes: {test4}")
    print(f"   DespuÃ©s: {normalize_text(test4)}")
    
    # Test 5: Caso real del PDF
    test5 = """El co llar de la rei na es una obra maes tra de Mau rice Le blanc , 
    pu bli ca da en 1907 . En ella , el au tor fran cÃ©s na rra las a ven tu ras"""
    print(f"\nğŸ“ Test 5 - Caso real (PDF con OCR malo):")
    print(f"   Antes: {test5}")
    print(f"   DespuÃ©s: {normalize_text(test5)}")
    
    # Test 6: DetecciÃ³n de errores
    print(f"\nğŸ” Test 6 - DetecciÃ³n de errores:")
    errors = detect_ocr_errors(test5)
    print(f"   Palabras fragmentadas: {errors['fragmented_words']}")
    print(f"   Guiones de separaciÃ³n: {errors['hyphen_breaks']}")
    print(f"   Espacios mÃºltiples: {errors['multiple_spaces']}")
    print(f"   Errores de puntuaciÃ³n: {errors['punctuation_spacing']}")
    print(f"   Â¿Tiene errores?: {'âœ… SÃ' if errors['has_errors'] else 'âŒ NO'}")
    
    print("\n" + "=" * 70)
    print("âœ… Tests completados")
    print("=" * 70)
