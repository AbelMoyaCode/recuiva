"""
Generador de Preguntas Inteligentes usando Groq API
Sistema RAG (Retrieval-Augmented Generation) para Active Recall

Este m√≥dulo genera preguntas de comprensi√≥n profunda bas√°ndose en los chunks
del material PDF usando Groq API (GRATIS 100% - Ultra r√°pido).

Autor: Abel Jes√∫s Moya Acosta
Fecha: 17 de noviembre de 2025
"""

import os
import json
import asyncio
from typing import List, Dict, Optional
from dotenv import load_dotenv
from groq import AsyncGroq

# Cargar variables de entorno
load_dotenv()

# Configuraci√≥n de Groq
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL = "llama-3.1-8b-instant"  # Llama 3.1 8B - Ultra r√°pido y sin l√≠mites de tokens


def classify_question_type(question: str) -> str:
    """
    Clasifica una pregunta como 'literal', 'inferential' u 'other'
    usando reglas simples basadas en palabras clave.
    
    - LITERAL: pide info expl√≠cita del texto (¬øqui√©n?, ¬øqu√© hizo?, ¬ød√≥nde?, ¬øcu√°ndo?, ¬øcu√°ntos?‚Ä¶)
    - INFERENCIAL: pide deducir, interpretar, explicar causas, intenciones, consecuencias
    
    Args:
        question: Texto de la pregunta
        
    Returns:
        'literal', 'inferential' u 'other'
    """
    q = question.lower().strip()
    
    # Patrones para preguntas INFERENCIALES (razonamiento, deducci√≥n)
    inferential_patterns = [
        # Por qu√© (causa/raz√≥n)
        "por qu√©", "por que",
        # Inferir/deducir
        "qu√© sugiere", "que sugiere",
        "qu√© podemos inferir", "que podemos inferir",
        "qu√© se puede inferir", "que se puede inferir",
        "qu√© puede inferirse", "que puede inferirse",
        "puede inferirse", "puede deducirse",
        "se puede inferir", "se puede deducir",
        # C√≥mo se + verbo (relaciones, comportamientos, explicaciones)
        "c√≥mo se relaciona", "como se relaciona",
        "c√≥mo se explica", "como se explica",
        "c√≥mo se comport√≥", "como se comporto",
        "c√≥mo se conecta", "como se conecta",
        "c√≥mo se vincula", "como se vincula",
        "c√≥mo se manifiesta", "como se manifiesta",
        "c√≥mo se refleja", "como se refleja",
        "c√≥mo se evidencia", "como se evidencia",
        "c√≥mo se caracteriza", "como se caracteriza",  # üëà NUEVO
        "c√≥mo se desarrolla", "como se desarrolla",  # üëà NUEVO
        "c√≥mo se transforma", "como se transforma",  # üëà NUEVO
        "c√≥mo se presenta", "como se presenta",  # üëà NUEVO
        "c√≥mo se describe", "como se describe",  # üëà NUEVO
        "c√≥mo reaccion√≥", "como reacciono",
        "c√≥mo actu√≥", "como actuo",
        "c√≥mo logr√≥", "como logro",
        "c√≥mo influy√≥", "como influyo",
        # Intenciones/consecuencias
        "qu√© intenci√≥n", "que intencion", "que intenci√≥n",
        "qu√© consecuencias", "que consecuencias",
        "qu√© implicaciones", "que implicaciones",
        # Opini√≥n/interpretaci√≥n
        "qu√© crees", "que crees",
        "qu√© piensas", "que piensas",
        "qu√© opinas", "que opinas",
        "c√≥mo interpretas", "como interpretas",
        # Significado/relaci√≥n
        "qu√© significa", "que significa",
        "qu√© relaci√≥n", "que relacion", "que relaci√≥n",
        "c√≥mo influye", "como influye",
        "qu√© motiva", "que motiva",
        # Causa/efecto
        "cu√°l es la causa", "cual es la causa",
        "cu√°l es el motivo", "cual es el motivo",
        "qu√© efecto", "que efecto",
        "c√≥mo afecta", "como afecta",
        # Hipot√©ticos
        "qu√© podr√≠a", "que podria", "que podr√≠a",
        "qu√© hubiera", "que hubiera",
        "qu√© habr√≠a", "que habria", "que habr√≠a",
        # An√°lisis
        "de qu√© manera", "de que manera",
        "en qu√© sentido", "en que sentido",
        "qu√© nos dice esto sobre", "que nos dice esto sobre",
        "qu√© revela", "que revela",
        "c√≥mo demuestra", "como demuestra",
        "qu√© demuestra", "que demuestra",
        "qu√© indica", "que indica",
        "qu√© evidencia", "que evidencia",
        "qu√© refleja", "que refleja",
        "qu√© nos permite", "que nos permite",
        "sobre su comprensi√≥n", "sobre su entendimiento",
        # Frases con "la idea de" (an√°lisis conceptual)
        "con la idea de", "la idea de",
        # Preguntas de an√°lisis profundo
        "qu√© papel juega", "que papel juega",
        "qu√© rol cumple", "que rol cumple",
        "qu√© funci√≥n tiene", "que funcion tiene",
        "qu√© importancia", "que importancia",
        # Preguntas evaluativas y comparativas
        "qu√© ventaja", "que ventaja",
        "qu√© desventaja", "que desventaja",
        "qu√© beneficio", "que beneficio",
        "qu√© diferencia", "que diferencia",
        "qu√© similitud", "que similitud",
        "qu√© aspecto", "que aspecto",
        "qu√© caracter√≠stica", "que caracteristica", "que caracter√≠stica",
        "qu√© elemento", "que elemento",
        "qu√© factor", "que factor",
        "qu√© rasgo", "que rasgo",
        "qu√© cualidad", "que cualidad",
        "qu√© tipo de", "que tipo de",
        # M√°s patrones de "c√≥mo se..."
        "c√≥mo se percibe", "como se percibe",
        "c√≥mo se representa", "como se representa",
        "c√≥mo se expresa", "como se expresa",
        "c√≥mo se construye", "como se construye",
        "c√≥mo se articula", "como se articula",
        "c√≥mo se plantea", "como se plantea",
        "c√≥mo se vincula", "como se vincula",
        "c√≥mo se conecta", "como se conecta",
        "c√≥mo se estructura", "como se estructura",
        "c√≥mo se organiza", "como se organiza",
        "c√≥mo se define", "como se define",
        "c√≥mo se ejemplifica", "como se ejemplifica",
        "c√≥mo se aplica", "como se aplica",
        "c√≥mo se usa", "como se usa",
        "c√≥mo se utiliza", "como se utiliza",
        # Preguntas de valoraci√≥n
        "es importante", "son importantes",
        "es significativo", "son significativos",
        "es relevante", "son relevantes"
    ]
    
    # Patrones para preguntas LITERALES (informaci√≥n expl√≠cita)
    literal_patterns = [
        "qui√©n ", "quien ",  # espacio para evitar falsos positivos
        "qui√©nes", "quienes",
        "qu√© hizo", "que hizo",
        "qu√© pas√≥", "que paso", "qu√© pas√≥",
        "qu√© ocurri√≥", "que ocurrio", "que ocurri√≥",
        "qu√© sucedi√≥", "que sucedio", "que sucedi√≥",
        "d√≥nde ", "donde ",
        "cu√°ndo", "cuando",
        "cu√°ntos", "cuantos",
        "cu√°ntas", "cuantas",
        "en qu√© a√±o", "en que a√±o", "en que ano",
        "en qu√© lugar", "en que lugar",
        "en qu√© ciudad", "en que ciudad",
        "en qu√© pa√≠s", "en que pais", "en que pa√≠s",
        "qu√© recibi√≥", "que recibio", "que recibi√≥",
        "qu√© encontr√≥", "que encontro", "que encontr√≥",
        "qu√© dijo", "que dijo",
        "qu√© respondi√≥", "que respondio", "que respondi√≥",
        "cu√°l es el nombre", "cual es el nombre",
        "c√≥mo se llama", "como se llama",
        "a qui√©n", "a quien",
        "de qui√©n", "de quien",
        "qu√© objeto", "que objeto",
        "qu√© color", "que color",
        "qu√© tipo de", "que tipo de",
        "cu√°l fue", "cual fue",
        "qu√© edad", "que edad",
        "cu√°nto tiempo", "cuanto tiempo",
        "cu√°nto dinero", "cuanto dinero",
        "qu√© cantidad", "que cantidad"
    ]
    
    # Verificar patrones inferenciales primero (m√°s espec√≠ficos)
    if any(pat in q for pat in inferential_patterns):
        return "inferential"
    
    # Luego verificar patrones literales
    if any(pat in q for pat in literal_patterns):
        return "literal"
    
    return "other"


async def generate_questions_with_ai(
    material_id: str,
    supabase_client,
    num_questions_per_chunk: int = 2,
    max_chunks: Optional[int] = None
) -> Dict:
    """
    Genera preguntas inteligentes usando Groq RAG
    
    Args:
        material_id: UUID del material en Supabase
        supabase_client: Cliente de Supabase
        num_questions_per_chunk: N√∫mero de preguntas por chunk (default: 2)
        max_chunks: L√≠mite de chunks a procesar (None = todos)
        
    Returns:
        Dict con:
            - success: bool
            - questions: List[Dict] con preguntas generadas
            - total_questions: int
            - chunks_processed: int
            - cost_estimate: float (USD estimado)
    """
    
    print(f"\n{'='*70}")
    print(f"  GENERANDO PREGUNTAS CON GROQ AI ({GROQ_MODEL})")
    print(f"{'='*70}")
    
    # 0. Validar que GROQ_API_KEY est√© configurada
    if not GROQ_API_KEY:
        print("‚ùå GROQ_API_KEY no est√° configurada en .env")
        return {
            "success": False,
            "error": "GROQ_API_KEY no est√° configurada. Config√∫rala en el archivo .env del servidor.",
            "questions": [],
            "total_questions": 0,
            "chunks_processed": 0
        }
    
    # 1. Obtener chunks del material desde Supabase
    print(f"üìö Obteniendo chunks del material {material_id}...")
    
    try:
        response = supabase_client.table('material_embeddings')\
            .select('id, chunk_index, chunk_text')\
            .eq('material_id', material_id)\
            .order('chunk_index')\
            .execute()
        
        chunks = response.data
        
        if not chunks:
            return {
                "success": False,
                "error": f"No se encontraron chunks para material {material_id}",
                "questions": [],
                "total_questions": 0,
                "chunks_processed": 0
            }
        
        # Limitar chunks si se especifica
        if max_chunks:
            chunks = chunks[:max_chunks]
        
        print(f"‚úÖ Chunks encontrados: {len(chunks)}")
        
    except Exception as e:
        return {
            "success": False,
            "error": f"Error obteniendo chunks: {str(e)}",
            "questions": [],
            "total_questions": 0,
            "chunks_processed": 0
        }
    
    # 2. Generar preguntas POR LOTES (batch processing) - 10x m√°s r√°pido
    all_questions = []
    chunks_processed = 0
    chunks_failed = 0
    
    # Configuraci√≥n de lotes
    BATCH_SIZE = 10  # Procesar 10 chunks a la vez
    total_batches = (len(chunks) + BATCH_SIZE - 1) // BATCH_SIZE
    
    print(f"\nü§ñ Generando preguntas con Groq AI (‚ö° ultra r√°pido)...")
    print(f"   Chunks a procesar: {len(chunks)}")
    print(f"   Tama√±o de lote: {BATCH_SIZE} chunks")
    print(f"   Total de lotes: {total_batches}")
    print(f"   Preguntas por chunk: {num_questions_per_chunk}")
    print(f"   Total esperado: {len(chunks) * num_questions_per_chunk} preguntas\n")
    
    # Procesar chunks en lotes
    for batch_num in range(total_batches):
        start_idx = batch_num * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(chunks))
        batch_chunks = chunks[start_idx:end_idx]
        
        print(f"   üì¶ Lote {batch_num + 1}/{total_batches} ({len(batch_chunks)} chunks)...", end=" ")
        
        try:
            # Generar preguntas para todo el lote en una sola llamada
            batch_questions = await generate_questions_batch(
                chunks_batch=batch_chunks,
                num_questions_per_chunk=num_questions_per_chunk
            )
            
            # Agregar preguntas generadas
            all_questions.extend(batch_questions)
            chunks_processed += len(batch_chunks)
            print(f"‚úÖ {len(batch_questions)} preguntas")
            
        except Exception as e:
            chunks_failed += len(batch_chunks)
            print(f"‚ùå Error: {str(e)[:50]}...")
            continue
    
    print(f"\n{'='*70}")
    print(f"  RESUMEN DE GENERACI√ìN")
    print(f"{'='*70}")
    print(f"‚úÖ Chunks procesados: {chunks_processed}/{len(chunks)}")
    if chunks_failed > 0:
        print(f"‚ö†Ô∏è  Chunks fallidos: {chunks_failed}")
    print(f"‚úÖ Preguntas generadas: {len(all_questions)}")
    print(f"üí∞ Costo: $0.00 (GRATIS 100%)")
    print(f"{'='*70}\n")
    
    return {
        "success": True,
        "questions": all_questions,
        "total_questions": len(all_questions),
        "chunks_processed": chunks_processed,
        "chunks_failed": chunks_failed,
        "cost_estimate": 0.0
    }


async def generate_questions_for_chunk(
    chunk_text: str,
    chunk_index: int,
    num_questions: int = 2
) -> List[str]:
    """
    Genera preguntas para un chunk espec√≠fico usando Groq
    
    Args:
        chunk_text: Texto del chunk
        chunk_index: √çndice del chunk (para contexto)
        num_questions: N√∫mero de preguntas a generar
        
    Returns:
        List[str]: Lista de preguntas generadas
    """
    
    # Prompt optimizado para Llama 3.1 70B
    system_prompt = """Eres un profesor universitario experto en Active Recall y pedagog√≠a.

Tu tarea: Generar preguntas de comprensi√≥n profunda para aprendizaje activo.

REGLAS ESTRICTAS:
1. Las preguntas DEBEN requerir EXPLICAR, ANALIZAR, COMPARAR o RELACIONAR conceptos (NO memorizar datos)
2. Basarse √öNICAMENTE en el contenido del fragmento proporcionado
3. Ser espec√≠ficas y contextualizadas al contenido
4. Usar terminolog√≠a acad√©mica apropiada
5. Fomentar pensamiento cr√≠tico y comprensi√≥n profunda

FORMATO DE SALIDA: JSON v√°lido con esta estructura:
{
  "questions": ["Pregunta 1", "Pregunta 2"]
}

Responde SOLO con el JSON, sin texto adicional."""

    user_prompt = f"""Fragmento del libro (Secci√≥n {chunk_index}):

{chunk_text}

Genera {num_questions} preguntas de Active Recall en formato JSON."""

    try:
        # Llamada a Groq API (AsyncGroq)
        client = AsyncGroq(api_key=GROQ_API_KEY)
        
        completion = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        
        # Parsear respuesta
        generated_text = completion.choices[0].message.content
        
        # Limpiar posibles marcadores de markdown
        if generated_text.startswith("```json"):
            generated_text = generated_text[7:]
        if generated_text.endswith("```"):
            generated_text = generated_text[:-3]
        
        # Parsear JSON
        generated = json.loads(generated_text.strip())
        
        # Validar estructura
        if "questions" not in generated:
            raise ValueError("Respuesta no contiene campo 'questions'")
        
        questions = generated["questions"]
        
        # Validar que sean strings no vac√≠as
        questions = [q.strip() for q in questions if isinstance(q, str) and q.strip()]
        
        return questions
        
    except json.JSONDecodeError as e:
        print(f"\n‚ö†Ô∏è  Error parseando JSON: {e}")
        return []
    
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Error llamando a Groq API: {e}")
        return []


async def generate_questions_batch(
    chunks_batch: List[Dict],
    num_questions_per_chunk: int = 2
) -> List[Dict]:
    """
    Genera preguntas para un lote de chunks en UNA SOLA llamada a Groq
    
    OPTIMIZACI√ìN: Procesa 10 chunks a la vez en lugar de uno por uno
    Esto reduce de 153 llamadas a 15 llamadas (10x m√°s r√°pido)
    
    Args:
        chunks_batch: Lista de chunks (cada uno con id, chunk_index, chunk_text)
        num_questions_per_chunk: Preguntas por chunk
        
    Returns:
        List[Dict]: Preguntas con metadatos
    """
    
    # Validar GROQ_API_KEY antes de llamar
    if not GROQ_API_KEY:
        print("‚ùå GROQ_API_KEY no configurada en generate_questions_batch")
        return []
    
    # Construir prompt para procesar TODO el lote
    # MEJORA GPT: Mezcla de preguntas literales (50%) e inferenciales (50%)
    system_prompt = """Eres un profesor universitario experto en Active Recall y pedagog√≠a.

TAREA: Generar preguntas variadas para M√öLTIPLES fragmentos de un texto.

REGLAS IMPORTANTES:
1. Para CADA fragmento, genera exactamente las preguntas solicitadas
2. MEZCLA de tipos de preguntas (aproximadamente 50/50):
   - LITERALES: Datos espec√≠ficos del texto (qu√©, qui√©n, cu√°ndo, d√≥nde, cu√°ntos)
   - INFERENCIALES: Requieren razonar, analizar causas, consecuencias, intenciones
3. Las preguntas deben ser espec√≠ficas al contenido de cada fragmento
4. Usar terminolog√≠a del texto original
5. Fomentar tanto comprensi√≥n factual como pensamiento cr√≠tico

EJEMPLOS:
- Literal: "¬øQu√© objeto recib√≠a Henriette cada a√±o como regalo?"
- Inferencial: "¬øPor qu√© crees que el personaje sospechaba del mayordomo?"

FORMATO JSON ESTRICTO:
{
  "chunks": [
    {
      "chunk_index": 0,
      "questions": ["Pregunta 1", "Pregunta 2"]
    },
    {
      "chunk_index": 1,
      "questions": ["Pregunta 1", "Pregunta 2"]
    }
  ]
}

Responde SOLO con JSON v√°lido, sin texto adicional ni marcadores markdown."""

    # Construir user_prompt con todos los chunks del lote
    chunks_text = ""
    for chunk in chunks_batch:
        chunks_text += f"\n--- FRAGMENTO {chunk['chunk_index']} ---\n{chunk['chunk_text']}\n"
    
    user_prompt = f"""Genera {num_questions_per_chunk} preguntas de Active Recall para cada uno de estos {len(chunks_batch)} fragmentos:

{chunks_text}

Formato JSON con array "chunks"."""

    try:
        client = AsyncGroq(api_key=GROQ_API_KEY)
        
        completion = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=4000,  # M√°s tokens para procesar lote
            response_format={"type": "json_object"}
        )
        
        response_text = completion.choices[0].message.content
        
        # MEJORA GPT: Limpiar posibles marcadores markdown (robustez)
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        response_text = response_text.strip()
        
        response_json = json.loads(response_text)
        
        # Mapear preguntas con metadatos
        all_questions = []
        chunks_data = response_json.get("chunks", [])
        
        for chunk_data in chunks_data:
            chunk_idx = chunk_data.get("chunk_index")
            questions = chunk_data.get("questions", [])
            
            # Encontrar el chunk original
            original_chunk = next((c for c in chunks_batch if c['chunk_index'] == chunk_idx), None)
            
            if original_chunk:
                for question_text in questions:
                    if isinstance(question_text, str) and question_text.strip():
                        # Clasificar tipo de pregunta (literal/inferential/other)
                        q_type = classify_question_type(question_text.strip())
                        
                        all_questions.append({
                            "question": question_text.strip(),
                            "question_type": q_type,  # üëà NUEVO: tipo de pregunta
                            "chunk_id": original_chunk['id'],
                            "chunk_index": original_chunk['chunk_index'],
                            "source_preview": original_chunk['chunk_text'][:150] + "..."
                        })
            else:
                # MEJORA GPT: Loguear cuando no se encuentra el chunk
                print(f"   ‚ö†Ô∏è chunk_index {chunk_idx} no encontrado en el lote original")
        
        return all_questions
        
    except json.JSONDecodeError as e:
        print(f"\n‚ùå Error parseando JSON en lote: {e}")
        print(f"   Respuesta recibida: {response_text[:200]}...")
        return []
    
    except Exception as e:
        print(f"\n‚ùå Error en lote: {e}")
        return []


async def save_generated_questions_to_supabase(
    questions: List[Dict],
    material_id: str,
    user_id: str,
    supabase_client
) -> Dict:
    """
    Guarda preguntas generadas en Supabase
    
    Args:
        questions: Lista de diccionarios con preguntas
        material_id: UUID del material
        user_id: UUID del usuario
        supabase_client: Cliente de Supabase
        
    Returns:
        Dict con resultado de la operaci√≥n
    """
    
    print(f"\nüíæ Guardando {len(questions)} preguntas en Supabase...")
    
    saved_count = 0
    failed_count = 0
    
    for q in questions:
        try:
            # Insertar en tabla questions
            supabase_client.table('questions').insert({
                'material_id': material_id,
                'user_id': user_id,
                'question_text': q['question'],
                'topic': None,  # Se puede agregar categorizaci√≥n despu√©s
                'difficulty': 'medium',  # Por defecto
                'expected_answer': None  # No tenemos respuesta esperada
            }).execute()
            
            saved_count += 1
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error guardando pregunta: {str(e)[:50]}...")
            failed_count += 1
            continue
    
    print(f"‚úÖ Preguntas guardadas: {saved_count}/{len(questions)}")
    if failed_count > 0:
        print(f"‚ö†Ô∏è  Preguntas fallidas: {failed_count}")
    
    return {
        "success": True,
        "saved_count": saved_count,
        "failed_count": failed_count
    }


async def test_groq_connection():
    """
    Prueba la conexi√≥n con Groq API
    
    Returns:
        Dict con resultado de la prueba
    """
    
    print("\nüîç Probando conexi√≥n con Groq API...")
    
    if not GROQ_API_KEY:
        print("‚ùå GROQ_API_KEY no est√° configurada en .env")
        return {"success": False, "error": "API key no configurada"}
    
    try:
        client = AsyncGroq(api_key=GROQ_API_KEY)
        
        completion = await client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {"role": "user", "content": "Responde solo con 'OK' si puedes leerme."}
            ],
            max_tokens=10
        )
        
        response = completion.choices[0].message.content
        
        print(f"‚úÖ Conexi√≥n exitosa con Groq")
        print(f"   Modelo: {GROQ_MODEL}")
        print(f"   Respuesta: {response}")
        
        return {
            "success": True,
            "message": "Conexi√≥n exitosa",
            "response": response
        }
        
    except Exception as e:
        print(f"‚ùå Error conectando con Groq: {e}")
        return {
            "success": False,
            "error": str(e)
        }


# Script de prueba
if __name__ == "__main__":
    async def main():
        """Prueba b√°sica del m√≥dulo"""
        
        # Test: Probar conexi√≥n
        await test_groq_connection()
        
        print("\n" + "="*70)
        print("  TEST: Generaci√≥n de pregunta de ejemplo")
        print("="*70)
        
        sample_chunk = """
        El collar de la reina es una obra maestra de Maurice Leblanc,
        publicada en 1907. En ella, el autor franc√©s narra las aventuras
        de Ars√®ne Lupin, el famoso ladr√≥n de guante blanco, quien se
        enfrenta a un enigma hist√≥rico relacionado con el collar de
        diamantes de la reina Mar√≠a Antonieta.
        """
        
        questions = await generate_questions_for_chunk(
            chunk_text=sample_chunk,
            chunk_index=0,
            num_questions=2
        )
        
        print(f"\n‚úÖ Preguntas generadas:\n")
        for i, q in enumerate(questions, 1):
            print(f"   {i}. {q}\n")
    
    # Ejecutar test
    asyncio.run(main())
