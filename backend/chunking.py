"""
M√≥dulo para chunking y extracci√≥n de texto
Divide textos largos en fragmentos manejables para embeddings

Autor: Abel Jes√∫s Moya Acosta
Fecha: 7 de octubre de 2025

‚úÖ ACTUALIZADO: Sistema con Tesseract OCR REAL
   - PRIMERO intenta Tesseract OCR (lee im√°genes, mejor calidad)
   - Si Tesseract falla, usa PyMuPDF o PyPDF2 como fallback
   - Normalizaci√≥n agresiva post-extracci√≥n
"""

import re
from typing import List, Tuple
from io import BytesIO
import os

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TESSERACT OCR - MEJOR CALIDAD (lee la imagen visual del PDF)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
TESSERACT_AVAILABLE = False

# Rutas posibles de Tesseract seg√∫n SO
TESSERACT_PATHS = [
    r"C:\Program Files\Tesseract-OCR\tesseract.exe",  # Windows
    "/usr/bin/tesseract",                              # Linux (Docker/Ubuntu)
    "/usr/local/bin/tesseract",                        # macOS Homebrew
]

try:
    import pytesseract
    from pdf2image import convert_from_bytes
    
    # Buscar Tesseract en las rutas conocidas
    tesseract_found = False
    for path in TESSERACT_PATHS:
        if os.path.exists(path):
            pytesseract.pytesseract.tesseract_cmd = path
            tesseract_found = True
            break
    
    # Si no se encontr√≥ en rutas conocidas, intentar usar el del PATH del sistema
    if not tesseract_found:
        # En Linux/Docker, tesseract suele estar en PATH
        import shutil
        tesseract_in_path = shutil.which("tesseract")
        if tesseract_in_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_in_path
            tesseract_found = True
    
    if tesseract_found:
        # Verificar que funciona
        version = pytesseract.get_tesseract_version()
        TESSERACT_AVAILABLE = True
        print(f"‚úÖ Tesseract OCR v{version} disponible (MEJOR CALIDAD)")
    else:
        print(f"‚ö†Ô∏è Tesseract no encontrado en rutas conocidas ni en PATH")
        
except ImportError as e:
    print(f"‚ö†Ô∏è pytesseract o pdf2image no disponible: {e}")
except Exception as e:
    print(f"‚ö†Ô∏è Error inicializando Tesseract: {e}")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FALLBACKS: PyMuPDF y PyPDF2
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    print("‚úÖ PyMuPDF disponible (fallback)")
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("‚ö†Ô∏è PyMuPDF no disponible")

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
    print("‚úÖ PyPDF2 disponible (fallback)")
except ImportError:
    PYPDF2_AVAILABLE = False
    print("‚ö†Ô∏è PyPDF2 no disponible")

# ‚úÖ Normalizador para limpiar chunks de errores OCR
try:
    from text_normalizer import normalize_text
    NORMALIZER_AVAILABLE = True
    print("‚úÖ text_normalizer cargado - chunks ser√°n normalizados")
except ImportError:
    NORMALIZER_AVAILABLE = False
    print("‚ö†Ô∏è text_normalizer no disponible")


def extract_with_tesseract(pdf_content: bytes) -> Tuple[str, int, int]:
    """
    Extrae texto usando Tesseract OCR REAL
    
    ‚úÖ OPTIMIZADO PARA MEMORIA BAJA (2GB VPS):
    - Procesa UNA p√°gina a la vez (no todas a memoria)
    - DPI reducido a 150 (suficiente para texto, menos RAM)
    - Libera memoria despu√©s de cada p√°gina
    - Para PDFs muy grandes, usa fallback autom√°tico
    
    MEJOR para PDFs con texto corrupto o escaneados.
    """
    import gc
    
    print("üîç Usando Tesseract OCR (mejor calidad)...")
    
    # Primero, obtener el n√∫mero total de p√°ginas sin cargar im√°genes
    try:
        # Usar PyMuPDF para contar p√°ginas (muy eficiente en memoria)
        if PYMUPDF_AVAILABLE:
            import fitz
            pdf_doc = fitz.open(stream=pdf_content, filetype="pdf")
            total_pages = len(pdf_doc)
            pdf_doc.close()
        else:
            # Fallback: convertir solo primera p√°gina para contar
            first_page = convert_from_bytes(pdf_content, dpi=72, first_page=1, last_page=1)
            total_pages = len(convert_from_bytes(pdf_content, dpi=72))
            del first_page
            gc.collect()
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error contando p√°ginas: {e}")
        total_pages = 0
    
    # Si el PDF es muy grande (>100 p√°ginas), usar DPI m√°s bajo o fallback
    if total_pages > 100:
        print(f"   ‚ö†Ô∏è PDF muy grande ({total_pages} p√°gs), usando DPI bajo (100) para ahorrar memoria")
        dpi = 100
    elif total_pages > 50:
        print(f"   üìÑ PDF mediano ({total_pages} p√°gs), usando DPI 150")
        dpi = 150
    else:
        print(f"   üìÑ PDF peque√±o ({total_pages} p√°gs), usando DPI 200")
        dpi = 200
    
    text = ""
    error_count = 0
    processed_pages = 0
    
    # Procesar p√°gina por p√°gina para ahorrar memoria
    try:
        for page_num in range(1, total_pages + 1):
            try:
                # Convertir SOLO esta p√°gina a imagen
                images = convert_from_bytes(
                    pdf_content, 
                    dpi=dpi, 
                    first_page=page_num, 
                    last_page=page_num,
                    grayscale=True,  # Menos memoria
                    thread_count=1   # Menos memoria
                )
                
                if images:
                    # Aplicar OCR con idioma espa√±ol
                    page_text = pytesseract.image_to_string(images[0], lang='spa+eng')
                    
                    # Contar posibles errores
                    error_count += len(re.findall(r'[a-z]{3,}[A-Z][a-z]{2,}', page_text))
                    error_count += len(re.findall(r'\b\w{1,2}\s+\w{1,2}\s+\w{1,2}\b', page_text))
                    
                    text += page_text + "\n\n"
                    processed_pages += 1
                    
                    # Liberar memoria inmediatamente
                    del images
                    del page_text
                
                # Log de progreso cada 10 p√°ginas
                if page_num % 10 == 0:
                    print(f"   OCR p√°gina {page_num}/{total_pages}...")
                    gc.collect()  # Forzar limpieza de memoria
                    
            except Exception as page_error:
                print(f"   ‚ö†Ô∏è Error en p√°gina {page_num}: {page_error}")
                continue
                
    except Exception as e:
        print(f"   ‚ùå Error general en Tesseract: {e}")
        if processed_pages == 0:
            raise e
    
    # Limpieza final
    gc.collect()
    
    print(f"   ‚úÖ Tesseract completado: {len(text)} caracteres de {processed_pages} p√°ginas")
    return text.strip(), processed_pages if processed_pages > 0 else total_pages, error_count


def extract_with_pymupdf(pdf_content: bytes) -> Tuple[str, int, int]:
    """Extrae texto con PyMuPDF"""
    pdf_document = fitz.open(stream=pdf_content, filetype="pdf")
    text = ""
    total_pages = len(pdf_document)
    error_count = 0
    
    for page in pdf_document:
        page_text = page.get_text("text")
        # Contar errores de OCR (palabras pegadas o fragmentadas)
        error_count += len(re.findall(r'[a-z]{3,}[A-Z][a-z]{2,}', page_text))  # palabrasPegadas
        error_count += len(re.findall(r'\b\w{1,2}\s+\w{1,2}\s+\w{1,2}\b', page_text))  # f ra g men tos
        text += page_text + "\n\n"
    
    pdf_document.close()
    return text.strip(), total_pages, error_count


def detect_corrupted_text(text: str) -> Tuple[bool, str]:
    """
    Detecta si el texto extra√≠do est√° corrupto (encoding incorrecto, fuentes propietarias, etc.)
    
    Args:
        text: Texto extra√≠do del PDF
        
    Returns:
        Tuple[bool, str]: (est√°_corrupto, raz√≥n)
    """
    if not text or len(text) < 100:
        return True, "Texto muy corto o vac√≠o"
    
    # 1. Detectar caracteres de control o no imprimibles
    control_chars = len(re.findall(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', text))
    if control_chars > len(text) * 0.01:  # M√°s de 1% caracteres de control
        return True, f"Demasiados caracteres de control ({control_chars})"
    
    # 2. Detectar secuencias de s√≠mbolos que indican encoding incorrecto
    # Ej: "‚Üí", "‚Üê", "‚Üî", "‚áí", etc. que aparecen en medio de palabras
    arrow_in_words = len(re.findall(r'\w[‚Üí‚Üê‚Üî‚áí‚áê‚Üë‚Üì]\w', text))
    if arrow_in_words > 5:
        return True, f"S√≠mbolos de flecha en palabras ({arrow_in_words})"
    
    # 3. Detectar ratio bajo de vocales (texto normal tiene ~40% vocales en espa√±ol)
    vowels = len(re.findall(r'[aeiou√°√©√≠√≥√∫AEIOU√Å√â√ç√ì√ö]', text))
    letters = len(re.findall(r'[a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë]', text))
    if letters > 0:
        vowel_ratio = vowels / letters
        if vowel_ratio < 0.25:  # Menos de 25% vocales = probablemente corrupto
            return True, f"Ratio de vocales muy bajo ({vowel_ratio:.1%})"
    
    # 4. Detectar palabras con mezcla inusual de may√∫sculas/min√∫sculas
    # Ej: "grofeso", "entradJ", "aToda"
    weird_case = len(re.findall(r'\b[a-z]+[A-Z][a-z]*\b', text))
    if weird_case > len(text.split()) * 0.05:  # M√°s de 5% de palabras
        return True, f"Mezcla inusual de may√∫sculas ({weird_case} palabras)"
    
    # 5. Detectar secuencias de caracteres raros consecutivos
    # Ej: ")El", "J‚Üí", etc.
    weird_sequences = len(re.findall(r'[)}\]>][A-Za-z]|[A-Za-z][(\[{<]', text))
    if weird_sequences > 20:
        return True, f"Secuencias de caracteres raros ({weird_sequences})"
    
    # 6. Detectar palabras que no parecen espa√±ol/ingl√©s
    # Palabras de >4 letras sin vocales
    no_vowel_words = len(re.findall(r'\b[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]{5,}\b', text))
    if no_vowel_words > 10:
        return True, f"Palabras sin vocales ({no_vowel_words})"
    
    # 7. ‚úÖ NUEVO: Detectar espacios insertados incorrectamente dentro de palabras
    # Patr√≥n: letra espacio 1-2 letras espacio letra (ej: "d e l a" o "rein a")
    fragmented_words = len(re.findall(r'\b[a-z√°√©√≠√≥√∫]\s[a-z√°√©√≠√≥√∫]{1,2}\s[a-z√°√©√≠√≥√∫]', text.lower()))
    if fragmented_words > 20:
        return True, f"Palabras fragmentadas por espacios ({fragmented_words})"
    
    # 8. ‚úÖ NUEVO: Detectar palabras pegadas sin espacios
    # Patr√≥n: secuencias muy largas de letras min√∫sculas (>25 caracteres sin espacio)
    glued_words = len(re.findall(r'[a-z√°√©√≠√≥√∫]{25,}', text.lower()))
    if glued_words > 10:
        return True, f"Palabras pegadas sin espacios ({glued_words})"
    
    # 9. ‚úÖ NUEVO: Detectar patr√≥n espec√≠fico de espaciado incorrecto
    # "del a" en lugar de "de la", "enwww" en lugar de "en www"
    bad_spacing_patterns = len(re.findall(r'\b(de|en|la|el|un|los|las|por|con)\s[a-z]\s', text.lower()))
    bad_spacing_patterns += len(re.findall(r'[a-z](www|http|com|org|net)', text.lower()))
    if bad_spacing_patterns > 15:
        return True, f"Espaciado incorrecto detectado ({bad_spacing_patterns})"
    
    return False, "Texto parece normal"


def repair_corrupted_spacing(text: str) -> str:
    """
    Repara texto con espaciado corrupto de PDFs con fuentes propietarias
    
    Problema: PyMuPDF extrae texto con espacios en lugares incorrectos
    Ejemplo: "de scargadoenwww. el ejandri a" ‚Üí "descargado en www. alejandr√≠a"
    
    Estrategia:
    1. Quitar TODOS los espacios
    2. Reconstruir usando diccionario de palabras espa√±olas
    3. Insertar espacios en los lugares correctos
    """
    if not text or len(text) < 50:
        return text
    
    # Palabras comunes del espa√±ol (para reconocer d√≥nde van los espacios)
    PALABRAS_COMUNES = {
        # Art√≠culos y determinantes
        'el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'al', 'del',
        # Preposiciones
        'a', 'ante', 'bajo', 'con', 'contra', 'de', 'desde', 'en', 'entre',
        'hacia', 'hasta', 'para', 'por', 'sin', 'sobre', 'tras',
        # Conjunciones
        'y', 'e', 'o', 'u', 'que', 'como', 'cuando', 'donde', 'si', 'ni', 'pero', 'sino', 'aunque',
        # Pronombres
        'yo', 'tu', 'el', 'ella', 'usted', 'nosotros', 'vosotros', 'ellos', 'ellas',
        'me', 'te', 'se', 'nos', 'os', 'lo', 'la', 'le', 'les',
        'mi', 'mis', 'tu', 'tus', 'su', 'sus', 'nuestro', 'nuestra', 'vuestro', 'vuestra',
        'este', 'esta', 'esto', 'estos', 'estas', 'ese', 'esa', 'eso', 'esos', 'esas',
        'aquel', 'aquella', 'aquello', 'aquellos', 'aquellas',
        'quien', 'quienes', 'cual', 'cuales', 'cuyo', 'cuya', 'cuyos', 'cuyas',
        # Verbos comunes
        'ser', 'estar', 'tener', 'haber', 'hacer', 'poder', 'decir', 'ir', 'ver', 'dar',
        'saber', 'querer', 'llegar', 'pasar', 'deber', 'poner', 'parecer', 'quedar', 'creer', 'llevar',
        'es', 'era', 'fue', 'son', 'eran', 'fueron', 'sido', 'siendo',
        'ha', 'he', 'has', 'han', 'hab√≠a', 'hab√≠an', 'hubo',
        'tiene', 'ten√≠a', 'tuvo', 'tienen', 'ten√≠an', 'tuvieron',
        # Adverbios
        'no', 'si', 'ya', 'aun', 'tan', 'muy', 'mas', 'menos', 'bien', 'mal',
        'aqui', 'alli', 'aca', 'alla', 'cerca', 'lejos', 'dentro', 'fuera', 'arriba', 'abajo',
        'antes', 'despues', 'luego', 'ahora', 'entonces', 'siempre', 'nunca', 'jamas',
        # Otras palabras muy comunes
        'todo', 'toda', 'todos', 'todas', 'otro', 'otra', 'otros', 'otras',
        'mismo', 'misma', 'mismos', 'mismas', 'solo', 'sola', 'solos', 'solas',
        'cada', 'poco', 'poca', 'pocos', 'pocas', 'mucho', 'mucha', 'muchos', 'muchas',
        'tanto', 'tanta', 'tantos', 'tantas', 'algo', 'alguien', 'alguno', 'alguna',
        'nada', 'nadie', 'ninguno', 'ninguna', 'cualquier', 'cualquiera',
        'vez', 'veces', 'cosa', 'cosas', 'tiempo', 'dia', 'dias', 'noche', 'noches',
        'hombre', 'mujer', 'persona', 'gente', 'mundo', 'vida', 'casa', 'parte',
        'mano', 'manos', 'ojo', 'ojos', 'cabeza', 'cuerpo', 'corazon',
        'a√±o', 'a√±os', 'mes', 'meses', 'hora', 'horas', 'momento',
        'se√±or', 'se√±ora', 'don', 'do√±a', 'conde', 'condesa', 'rey', 'reina',
        'libro', 'collar', 'joya', 'joyas', 'diamante', 'diamantes',
    }
    
    # Patrones espec√≠ficos de correcci√≥n
    corrections = [
        # Espacios dentro de palabras comunes
        (r'\bde\s+l\s*a\b', 'de la'),
        (r'\bde\s+l\s*os\b', 'de los'),
        (r'\bde\s+l\s*as\b', 'de las'),
        (r'\be\s*l\s+a\b', 'el a'),  # cuidado: puede ser "el a" leg√≠timo
        (r'\bl\s*a\s+con\b', 'la con'),
        (r'\bcon\s+de\s*sa\b', 'condesa'),
        (r'\bco\s*llar\b', 'collar'),
        (r'\brei\s*n\s*a\b', 'reina'),
        (r'\bse\s*√±or\b', 'se√±or'),
        (r'\bse\s*√±ora\b', 'se√±ora'),
        (r'\bhab\s*√≠\s*a\b', 'hab√≠a'),
        (r'\bten\s*√≠\s*a\b', 'ten√≠a'),
        (r'\bquer\s*√≠\s*a\b', 'quer√≠a'),
        (r'\bpod\s*√≠\s*a\b', 'pod√≠a'),
        (r'\bdec\s*√≠\s*a\b', 'dec√≠a'),
        (r'\bhac\s*√≠\s*a\b', 'hac√≠a'),
        (r'\bven\s*√≠\s*a\b', 'ven√≠a'),
        (r'\bsal\s*√≠\s*a\b', 'sal√≠a'),
        (r'\bent\s*r\s*a\s*da\b', 'entrada'),
        (r'\bvent\s*ana\b', 'ventana'),
        (r'\bhab\s*it\s*aci\s*√≥n\b', 'habitaci√≥n'),
        (r'\bembaj\s*ada\b', 'embajada'),
        (r'\bhist√≥r\s*ic\s*a\b', 'hist√≥rica'),
        (r'\bmaravi\s*lloso\b', 'maravilloso'),
        (r'\ble\s*gend\s*ario\b', 'legendario'),
        (r'\bfam\s*oso\b', 'famoso'),
        (r'\bblan\s*cos\b', 'blancos'),
        (r'\bhom\s*bros\b', 'hombros'),
        
        # Palabras pegadas comunes
        (r'\bdela\b', 'de la'),
        (r'\bdelos\b', 'de los'),
        (r'\bdelas\b', 'de las'),
        (r'\benla\b', 'en la'),
        (r'\benlos\b', 'en los'),
        (r'\benlas\b', 'en las'),
        (r'\bconla\b', 'con la'),
        (r'\bconlos\b', 'con los'),
        (r'\bconlas\b', 'con las'),
        (r'\bporla\b', 'por la'),
        (r'\bporlos\b', 'por los'),
        (r'\bporlas\b', 'por las'),
        (r'\bparala\b', 'para la'),
        (r'\bparalos\b', 'para los'),
        (r'\bparalas\b', 'para las'),
        (r'\bsinla\b', 'sin la'),
        (r'\bsinlos\b', 'sin los'),
        (r'\bsinlas\b', 'sin las'),
        (r'\bqueera\b', 'que era'),
        (r'\bquees\b', 'que es'),
        (r'\bqueno\b', 'que no'),
        (r'\bquese\b', 'que se'),
        (r'\byque\b', 'y que'),
        (r'\byno\b', 'y no'),
        (r'\byel\b', 'y el'),
        (r'\byla\b', 'y la'),
        (r'\bylos\b', 'y los'),
        (r'\bylas\b', 'y las'),
        (r'\bose\b(?!a)', 'o se'),  # evitar "osea"
        (r'\basu\b', 'a su'),
        (r'\balsu\b', 'al su'),
        (r'\bselo\b', 'se lo'),
        (r'\bsela\b', 'se la'),
        (r'\bnosolo\b', 'no solo'),
        (r'\bsinoqu\s*e\b', 'sino que'),
        (r'\bm√°sbi\s*en\b', 'm√°s bien'),
        (r'\bsinembargo\b', 'sin embargo'),
        (r'\bas√≠que\b', 'as√≠ que'),
        (r'\btalcomo\b', 'tal como'),
        (r'\benefecto\b', 'en efecto'),
        (r'\bporsupuesto\b', 'por supuesto'),
        
        # URLs y dominios
        (r'enwww', 'en www'),
        (r'www\.\s*', 'www.'),
        (r'\.\s*com\b', '.com'),
        (r'\.\s*org\b', '.org'),
        (r'\.\s*net\b', '.net'),
    ]
    
    # Aplicar correcciones
    for pattern, replacement in corrections:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # Limpiar espacios m√∫ltiples
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text


def extract_with_pypdf2(pdf_content: bytes) -> Tuple[str, int, int]:
    """Extrae texto con PyPDF2"""
    pdf_file = BytesIO(pdf_content)
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    total_pages = len(pdf_reader.pages)
    error_count = 0
    
    for page in pdf_reader.pages:
        page_text = page.extract_text() or ""
        error_count += len(re.findall(r'[a-z]{3,}[A-Z][a-z]{2,}', page_text))
        error_count += len(re.findall(r'\b\w{1,2}\s+\w{1,2}\s+\w{1,2}\b', page_text))
        text += page_text + "\n"
    
    return text.strip(), total_pages, error_count


def extract_text_from_pdf(pdf_content: bytes) -> tuple[str, int]:
    """
    Extrae texto de un archivo PDF usando el mejor m√©todo disponible
    
    ‚úÖ ESTRATEGIA REVISADA - TESSERACT PRIMERO:
    
    El problema: PDFs con fuentes propietarias producen texto corrupto con PyMuPDF
    porque las fuentes tienen mapeo de caracteres incorrecto.
    
    Soluci√≥n: SIEMPRE usar Tesseract OCR primero (lee la imagen visual),
    solo usar PyMuPDF como fallback si Tesseract falla.
    
    Args:
        pdf_content: Contenido del PDF en bytes
        
    Returns:
        tuple: (texto extra√≠do, n√∫mero total de p√°ginas)
    """
    import gc
    
    results = []
    total_pages = 0
    
    print(f"üìñ Extrayendo texto del PDF...")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 1: Contar p√°ginas primero (r√°pido con PyMuPDF)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if PYMUPDF_AVAILABLE:
        try:
            import fitz
            pdf_doc = fitz.open(stream=pdf_content, filetype="pdf")
            total_pages = len(pdf_doc)
            pdf_doc.close()
            print(f"   üìÑ PDF tiene {total_pages} p√°ginas")
        except Exception as e:
            print(f"   ‚ö†Ô∏è No se pudo contar p√°ginas: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 2: TESSERACT OCR PRIMERO (mejor calidad, lee imagen visual)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if TESSERACT_AVAILABLE:
        try:
            print(f"   üîç Usando Tesseract OCR (lee imagen visual del PDF)...")
            text_tess, pages_tess, errors_tess = extract_with_tesseract(pdf_content)
            
            # Verificar que Tesseract produjo texto v√°lido
            if len(text_tess.strip()) > 100:
                is_corrupted, reason = detect_corrupted_text(text_tess)
                if not is_corrupted:
                    results.append(('Tesseract', text_tess, pages_tess, errors_tess))
                    total_pages = pages_tess
                    print(f"   ‚úÖ Tesseract: {len(text_tess)} chars, texto limpio")
                    
                    # Usar Tesseract directamente
                    text = aggressive_text_cleanup(text_tess)
                    gc.collect()
                    print(f"‚úÖ Texto extra√≠do con OCR: {len(text)} caracteres de {pages_tess} p√°ginas")
                    return text, pages_tess
                else:
                    print(f"   ‚ö†Ô∏è Tesseract produjo texto con problemas: {reason}")
                    results.append(('Tesseract', text_tess, pages_tess, errors_tess))
            else:
                print(f"   ‚ö†Ô∏è Tesseract produjo muy poco texto ({len(text_tess)} chars)")
                
        except Exception as e:
            print(f"   ‚ùå Tesseract fall√≥: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 3: PyMuPDF como FALLBACK (solo si Tesseract fall√≥)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if PYMUPDF_AVAILABLE and not results:
        try:
            print(f"   üìñ Fallback a PyMuPDF...")
            text_mupdf, pages_mupdf, errors_mupdf = extract_with_pymupdf(pdf_content)
            results.append(('PyMuPDF', text_mupdf, pages_mupdf, errors_mupdf))
            total_pages = pages_mupdf
            print(f"   PyMuPDF: {len(text_mupdf)} chars, {errors_mupdf} errores")
        except Exception as e:
            print(f"   ‚ùå PyMuPDF fall√≥: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 4: PyPDF2 como √∫ltimo recurso
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if not results and PYPDF2_AVAILABLE:
        try:
            print(f"   üìÑ √öltimo recurso: PyPDF2...")
            text_pypdf2, pages_pypdf2, errors_pypdf2 = extract_with_pypdf2(pdf_content)
            results.append(('PyPDF2', text_pypdf2, pages_pypdf2, errors_pypdf2))
            total_pages = pages_pypdf2
            print(f"   PyPDF2: {len(text_pypdf2)} chars")
        except Exception as e:
            print(f"   ‚ùå PyPDF2 fall√≥: {e}")
    
    if not results:
        raise Exception("No se pudo extraer texto del PDF con ning√∫n m√©todo")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ELEGIR EL MEJOR RESULTADO (el de menos errores)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    best = min(results, key=lambda x: x[3])  # x[3] = error_count
    print(f"   ‚úÖ Usando {best[0]}")
    
    text = best[1]
    total_pages = best[2]
    
    # Limpiar texto extra√≠do
    text = aggressive_text_cleanup(text)
    
    # Limpiar memoria
    gc.collect()
    
    print(f"‚úÖ Texto extra√≠do: {len(text)} caracteres de {total_pages} p√°ginas")
    return text, total_pages


def aggressive_text_cleanup(text: str) -> str:
    """
    Limpieza AGRESIVA de texto extra√≠do de PDF
    
    ‚úÖ MEJORADO: Repara texto con espaciado incorrecto de PyMuPDF
    
    Problema detectado: PDFs con fuentes propietarias producen:
    - "de scargadoenwww" ‚Üí "descargado en www"
    - "el ejandri a" ‚Üí "alejandr√≠a"
    - "lacon desade" ‚Üí "la condesa de"
    """
    if not text:
        return text
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 0: Reparaci√≥n de espaciado corrupto (NUEVO)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    text = repair_corrupted_spacing(text)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 1: Separar palabras pegadas (camelCase accidental)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # "serreconocido" ‚Üí "ser reconocido" (min√∫scula seguida de may√∫scula)
    text = re.sub(r'([a-z√°√©√≠√≥√∫√±])([A-Z√Å√â√ç√ì√ö√ë])', r'\1 \2', text)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 2: Unir fragmentos sueltos (errores OCR t√≠picos)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Patr√≥n: "palabra + espacio + 1-3 letras" ‚Üí unir
    # Ej: "histori a" ‚Üí "historia", "Henriet te" ‚Üí "Henriette"
    for _ in range(5):  # Repetir varias veces para casos anidados
        text = re.sub(r'(\w{3,})\s+([a-z√°√©√≠√≥√∫√±]{1,3})\b', r'\1\2', text, flags=re.IGNORECASE)
    
    # Patr√≥n: "1-4 letras + espacio + palabra" ‚Üí unir
    # Ej: "a doptar" ‚Üí "adoptar"
    for _ in range(3):
        text = re.sub(r'\b([a-z√°√©√≠√≥√∫√±]{1,4})\s+([a-z√°√©√≠√≥√∫√±]{3,})', r'\1\2', text, flags=re.IGNORECASE)
    
    # Patr√≥n: May√∫scula + espacio + resto
    # Ej: "V alorbe" ‚Üí "Valorbe"
    text = re.sub(r'\b([A-Z√Å√â√ç√ì√ö√ë])\s+([a-z√°√©√≠√≥√∫√±]{2,})', r'\1\2', text)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 3: Separar palabras que deber√≠an estar separadas
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Art√≠culos pegados a palabras
    articles = ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'al', 'del']
    for art in articles:
        # "losdem√°s" ‚Üí "los dem√°s"
        text = re.sub(rf'\b({art})([a-z√°√©√≠√≥√∫√±]{{3,}})', rf'\1 \2', text, flags=re.IGNORECASE)
    
    # Preposiciones pegadas
    preps = ['con', 'en', 'de', 'por', 'para', 'sin', 'sobre', 'entre', 'hasta', 'desde', 'como', 'que']
    for prep in preps:
        text = re.sub(rf'\b({prep})([a-z√°√©√≠√≥√∫√±]{{3,}})', rf'\1 \2', text, flags=re.IGNORECASE)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 4: Limpiar puntuaci√≥n y espacios
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Unir palabras cortadas por gui√≥n al final de l√≠nea
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)
    
    # M√∫ltiples espacios ‚Üí uno solo
    text = re.sub(r'[ \t]+', ' ', text)
    
    # M√∫ltiples saltos de l√≠nea ‚Üí m√°ximo 2
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # Espacios antes de puntuaci√≥n
    text = re.sub(r'\s+([.,;:!?])', r'\1', text)
    
    # Espacio despu√©s de puntuaci√≥n si falta
    text = re.sub(r'([.,;:!?])([a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë¬ø¬°])', r'\1 \2', text)
    
    # Remover l√≠neas que solo tienen n√∫meros (paginaci√≥n)
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
    
    return text.strip()

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """
    Divide el texto en chunks con overlap para mantener contexto
    
    OPTIMIZADO PARA PDFs DE 25-100+ P√ÅGINAS:
    - chunk_size=1000: Contexto completo (5-7 oraciones)
    - overlap=200: Mayor continuidad entre chunks
    
    Args:
        text: Texto a dividir
        chunk_size: Tama√±o aproximado de cada chunk (en caracteres) [default: 1000]
        overlap: Cantidad de caracteres que se solapan entre chunks [default: 200]
        
    Returns:
        List[str]: Lista de chunks de texto
    """
    # Limpiar el texto
    text = clean_text(text)
    
    # Dividir por oraciones
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        # Si agregar esta oraci√≥n excede el tama√±o, guardar el chunk actual
        if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            
            # Comenzar nuevo chunk con overlap
            words = current_chunk.split()
            overlap_words = min(overlap, len(words))
            overlap_text = ' '.join(words[-overlap_words:]) if overlap_words > 0 else ""
            current_chunk = overlap_text + " " + sentence if overlap_text else sentence
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    # Agregar el √∫ltimo chunk
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # ‚úÖ NUEVO: Normalizar todos los chunks para corregir errores OCR
    if NORMALIZER_AVAILABLE:
        chunks = [normalize_text(chunk) for chunk in chunks]
        print(f"‚úÖ Chunks normalizados: errores OCR corregidos")
    
    return chunks

def clean_text(text: str) -> str:
    """
    Limpia el texto removiendo caracteres innecesarios
    
    NOTA IMPORTANTE: 
    - Si el PDF tiene OCR defectuoso (espacios en medio de palabras), 
      este filtro NO lo arreglar√° autom√°ticamente
    - Para PDFs con OCR corrupto, ejecutar manualmente el script SQL:
      database/fix_ocr_chunks_CORRECTO.sql
    
    Args:
        text: Texto a limpiar
        
    Returns:
        str: Texto limpio
    """
    # Remover m√∫ltiples espacios
    text = re.sub(r'\s+', ' ', text)
    
    # Remover caracteres especiales pero mantener puntuaci√≥n b√°sica
    text = re.sub(r'[^\w\s.,;:!?¬ø¬°√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë()"\'-]', '', text)
    
    # Remover l√≠neas vac√≠as m√∫ltiples
    text = re.sub(r'\n\s*\n', '\n', text)
    
    return text.strip()

def get_text_stats(text: str, real_pages: int = None) -> dict:
    """
    Obtiene estad√≠sticas del texto
    
    Args:
        text: Texto a analizar
        real_pages: N√∫mero real de p√°ginas del PDF (si est√° disponible)
        
    Returns:
        dict: Diccionario con estad√≠sticas
    """
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    paragraphs = text.split('\n\n')
    
    # Calcular p√°ginas estimadas bas√°ndose en caracteres (1300 chars/p√°gina es m√°s realista)
    estimated_pages = len(text) // 1300 if not real_pages else real_pages
    
    return {
        "characters": len(text),
        "characters_no_spaces": len(text.replace(' ', '')),
        "words": len(words),
        "sentences": len([s for s in sentences if s.strip()]),
        "paragraphs": len([p for p in paragraphs if p.strip()]),
        "avg_word_length": round(sum(len(word) for word in words) / len(words), 2) if words else 0,
        "avg_sentence_length": round(len(words) / len([s for s in sentences if s.strip()]), 2) if sentences else 0,
        "estimated_pages": estimated_pages,
        "real_pages": real_pages if real_pages else estimated_pages  # Devolver el conteo real si existe
    }

def chunk_by_paragraphs(text: str, max_chunk_size: int = 1000) -> List[str]:
    """
    Divide el texto en chunks bas√°ndose en p√°rrafos
    
    Args:
        text: Texto a dividir
        max_chunk_size: Tama√±o m√°ximo de cada chunk
        
    Returns:
        List[str]: Lista de chunks
    """
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
        
        if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def smart_chunk(text: str, target_size: int = 500, min_size: int = 100) -> List[str]:
    """
    Chunking inteligente que respeta l√≠mites de oraciones y p√°rrafos
    
    Args:
        text: Texto a dividir
        target_size: Tama√±o objetivo de cada chunk
        min_size: Tama√±o m√≠nimo aceptable
        
    Returns:
        List[str]: Lista de chunks optimizados
    """
    text = clean_text(text)
    
    # Dividir por p√°rrafos primero
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        # Si el p√°rrafo es muy grande, dividirlo por oraciones
        if len(paragraph) > target_size:
            sentences = re.split(r'(?<=[.!?])\s+', paragraph)
            
            for sentence in sentences:
                if len(current_chunk) + len(sentence) > target_size and len(current_chunk) >= min_size:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    current_chunk += " " + sentence if current_chunk else sentence
        else:
            # Agregar p√°rrafo completo
            if len(current_chunk) + len(paragraph) > target_size and len(current_chunk) >= min_size:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    # Agregar el √∫ltimo chunk
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # ‚úÖ NUEVO: Normalizar todos los chunks para corregir errores OCR
    if NORMALIZER_AVAILABLE:
        print(f"üßπ Normalizando {len(chunks)} chunks (corrigiendo errores OCR)...")
        chunks = [normalize_text(chunk) for chunk in chunks]
        print(f"‚úÖ Chunks normalizados correctamente")
    
    return chunks


def adaptive_chunking(text: str, total_pages: int) -> List[str]:
    """
    üéØ CHUNKING ADAPTATIVO INTELIGENTE seg√∫n tama√±o del PDF
    
    Ajusta autom√°ticamente los par√°metros de chunking para mantener
    el equilibrio entre precisi√≥n y eficiencia seg√∫n el tama√±o del documento.
    
    ESTRATEGIA POR TAMA√ëO:
    üìò PDFs peque√±os (1-50 p√°gs):    chunks detallados (80-180 palabras)
    üìó PDFs medianos (51-300 p√°gs):  chunks moderados (150-350 palabras)
    üìï PDFs grandes (301-1000 p√°gs): chunks amplios (250-600 palabras)
    üìö PDFs masivos (1000+ p√°gs):    chunks extensos (400-1000 palabras)
    
    BENEFICIOS:
    ‚úÖ Reduce ruido en PDFs grandes (menos chunks = mejor retrieval)
    ‚úÖ Mantiene detalle en PDFs peque√±os
    ‚úÖ Optimiza tiempo de procesamiento
    ‚úÖ Mejor balance precisi√≥n/escalabilidad
    
    Args:
        text: Texto completo a dividir
        total_pages: N√∫mero total de p√°ginas del PDF
        
    Returns:
        List[str]: Chunks optimizados seg√∫n tama√±o del documento
    """
    print(f"\nüéØ CHUNKING ADAPTATIVO para PDF de {total_pages} p√°ginas")
    
    if total_pages <= 50:
        # PDFs peque√±os: m√°ximo detalle
        print("   üìò Estrategia: DETALLADA (80-180 palabras/chunk)")
        return semantic_chunking(text, min_words=80, max_words=180, overlap_words=20)
    
    elif total_pages <= 300:
        # PDFs medianos: balance detalle/eficiencia
        print("   üìó Estrategia: MODERADA (150-350 palabras/chunk)")
        return semantic_chunking(text, min_words=150, max_words=350, overlap_words=30)
    
    elif total_pages <= 1000:
        # PDFs grandes: priorizar coherencia
        print("   üìï Estrategia: AMPLIA (250-600 palabras/chunk)")
        return semantic_chunking(text, min_words=250, max_words=600, overlap_words=50)
    
    else:
        # PDFs masivos: reducir ruido al m√°ximo
        print("   üìö Estrategia: EXTENSIVA (400-1000 palabras/chunk)")
        return semantic_chunking(text, min_words=400, max_words=1000, overlap_words=80)

def semantic_chunking(text: str, min_words: int = 150, max_words: int = 400, overlap_words: int = 15) -> List[str]:
    """
    üß† CHUNKING SEM√ÅNTICO INTELIGENTE - BASE
    
    Divide texto por P√ÅRRAFOS Y ORACIONES (no caracteres arbitrarios).
    Se adapta al contenido respetando l√≠mites sem√°nticos.
    
    CARACTER√çSTICAS:
    ‚úÖ Divisi√≥n por p√°rrafos (\n\n) - respeta estructura del documento
    ‚úÖ Subdivisi√≥n por oraciones si p√°rrafo es muy largo
    ‚úÖ Context anchors: 15 palabras de overlap entre chunks
    ‚úÖ Rango adaptativo: 150-400 palabras (no caracteres fijos)
    ‚úÖ Respeta l√≠mites de ideas completas
    
    EJEMPLO DE RESULTADO:
    - Chunk antiguo (1000 chars): "...una amiga de convento que se enemis..." (cortado)
    - Chunk sem√°ntico (250 palabras): "En el edificio viv√≠a una amiga de convento que se enemist√≥ 
      con su familia. Prestaba servicios a la condesa y conoc√≠a sus rutinas. Siempre se hablaba 
      delante de ella. Su ventana de cocina daba exactamente al mismo patio interior..." (completo)
    
    Args:
        text: Texto completo a dividir
        min_words: M√≠nimo de palabras por chunk (default: 150)
        max_words: M√°ximo de palabras por chunk (default: 400)
        overlap_words: Palabras de overlap entre chunks (default: 15)
        
    Returns:
        List[str]: Chunks sem√°nticos con context anchors
    """
    text = clean_text(text)
    
    # 1. DIVIDIR POR P√ÅRRAFOS (respeta estructura del documento)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = []
    word_count = 0
    
    print(f"\nüß† INICIANDO CHUNKING SEM√ÅNTICO...")
    print(f"   Rango: {min_words}-{max_words} palabras por chunk")
    print(f"   Context anchors: {overlap_words} palabras de overlap")
    
    for paragraph in paragraphs:
        paragraph_words = paragraph.split()
        paragraph_word_count = len(paragraph_words)
        
        # Si el p√°rrafo es muy largo (> max_words), dividirlo por oraciones
        if paragraph_word_count > max_words:
            sentences = re.split(r'(?<=[.!?])\s+', paragraph)
            
            for sentence in sentences:
                sentence_words = sentence.split()
                sentence_word_count = len(sentence_words)
                
                # Si agregar esta oraci√≥n supera max_words, guardar chunk actual
                if word_count + sentence_word_count > max_words and word_count >= min_words:
                    # Guardar chunk actual
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text)
                    
                    # Context anchor: √∫ltimas N palabras del chunk anterior
                    overlap = current_chunk[-overlap_words:] if len(current_chunk) >= overlap_words else current_chunk
                    current_chunk = overlap + sentence_words
                    word_count = len(current_chunk)
                else:
                    current_chunk.extend(sentence_words)
                    word_count += sentence_word_count
        else:
            # P√°rrafo completo cabe en el chunk actual
            if word_count + paragraph_word_count > max_words and word_count >= min_words:
                # Guardar chunk actual
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text)
                
                # Context anchor
                overlap = current_chunk[-overlap_words:] if len(current_chunk) >= overlap_words else current_chunk
                current_chunk = overlap + paragraph_words
                word_count = len(current_chunk)
            else:
                # Agregar p√°rrafo al chunk actual
                if current_chunk:
                    current_chunk.append('\n\n')
                current_chunk.extend(paragraph_words)
                word_count += paragraph_word_count
    
    # Agregar √∫ltimo chunk
    if current_chunk:
        chunk_text = ' '.join(current_chunk)
        chunks.append(chunk_text)
    
    # Normalizar chunks
    if NORMALIZER_AVAILABLE:
        print(f"üßπ Normalizando {len(chunks)} chunks sem√°nticos...")
        chunks = [normalize_text(chunk) for chunk in chunks]
    
    # Estad√≠sticas
    chunk_lengths = [len(chunk.split()) for chunk in chunks]
    avg_words = sum(chunk_lengths) / len(chunks) if chunks else 0
    
    print(f"\n‚úÖ CHUNKING SEM√ÅNTICO COMPLETADO:")
    print(f"   Total chunks: {len(chunks)}")
    print(f"   Promedio palabras/chunk: {avg_words:.1f}")
    print(f"   Rango: {min(chunk_lengths)}-{max(chunk_lengths)} palabras")
    print(f"   Context anchors: {overlap_words} palabras de overlap\n")
    
    return chunks
