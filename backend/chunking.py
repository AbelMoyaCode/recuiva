"""
M√≥dulo para chunking y extracci√≥n de texto
Divide textos largos en fragmentos manejables para embeddings

Autor: Abel Jes√∫s Moya Acosta
Fecha: 7 de octubre de 2025

‚úÖ ACTUALIZADO: Sistema con Tesseract OCR REAL
   - PRIMERO intenta Tesseract OCR (lee im√°genes, mejor calidad)
   - Si Tesseract falla, usa PyMuPDF o PyPDF2 como fallback
   - Normalizaci√≥n agresiva post-extracci√≥n
"""

import re
from typing import List, Tuple
from io import BytesIO
import os

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# TESSERACT OCR - MEJOR CALIDAD (lee la imagen visual del PDF)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
TESSERACT_AVAILABLE = False

# Rutas posibles de Tesseract seg√∫n SO
TESSERACT_PATHS = [
    r"C:\Program Files\Tesseract-OCR\tesseract.exe",  # Windows
    "/usr/bin/tesseract",                              # Linux (Docker/Ubuntu)
    "/usr/local/bin/tesseract",                        # macOS Homebrew
]

try:
    import pytesseract
    from pdf2image import convert_from_bytes
    
    # Buscar Tesseract en las rutas conocidas
    tesseract_found = False
    for path in TESSERACT_PATHS:
        if os.path.exists(path):
            pytesseract.pytesseract.tesseract_cmd = path
            tesseract_found = True
            break
    
    # Si no se encontr√≥ en rutas conocidas, intentar usar el del PATH del sistema
    if not tesseract_found:
        # En Linux/Docker, tesseract suele estar en PATH
        import shutil
        tesseract_in_path = shutil.which("tesseract")
        if tesseract_in_path:
            pytesseract.pytesseract.tesseract_cmd = tesseract_in_path
            tesseract_found = True
    
    if tesseract_found:
        # Verificar que funciona
        version = pytesseract.get_tesseract_version()
        TESSERACT_AVAILABLE = True
        print(f"‚úÖ Tesseract OCR v{version} disponible (MEJOR CALIDAD)")
    else:
        print(f"‚ö†Ô∏è Tesseract no encontrado en rutas conocidas ni en PATH")
        
except ImportError as e:
    print(f"‚ö†Ô∏è pytesseract o pdf2image no disponible: {e}")
except Exception as e:
    print(f"‚ö†Ô∏è Error inicializando Tesseract: {e}")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# FALLBACKS: PyMuPDF y PyPDF2
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
try:
    import fitz  # PyMuPDF
    PYMUPDF_AVAILABLE = True
    print("‚úÖ PyMuPDF disponible (fallback)")
except ImportError:
    PYMUPDF_AVAILABLE = False
    print("‚ö†Ô∏è PyMuPDF no disponible")

try:
    import PyPDF2
    PYPDF2_AVAILABLE = True
    print("‚úÖ PyPDF2 disponible (fallback)")
except ImportError:
    PYPDF2_AVAILABLE = False
    print("‚ö†Ô∏è PyPDF2 no disponible")

# ‚úÖ Normalizador para limpiar chunks de errores OCR
try:
    from text_normalizer import normalize_text
    NORMALIZER_AVAILABLE = True
    print("‚úÖ text_normalizer cargado - chunks ser√°n normalizados")
except ImportError:
    NORMALIZER_AVAILABLE = False
    print("‚ö†Ô∏è text_normalizer no disponible")


def extract_with_tesseract(pdf_content: bytes) -> Tuple[str, int, int]:
    """
    Extrae texto usando Tesseract OCR REAL
    
    ‚úÖ OPTIMIZADO PARA MEMORIA BAJA (2GB VPS):
    - Procesa UNA p√°gina a la vez (no todas a memoria)
    - DPI reducido a 150 (suficiente para texto, menos RAM)
    - Libera memoria despu√©s de cada p√°gina
    - Para PDFs muy grandes, usa fallback autom√°tico
    
    MEJOR para PDFs con texto corrupto o escaneados.
    """
    import gc
    
    print("üîç Usando Tesseract OCR (mejor calidad)...")
    
    # Primero, obtener el n√∫mero total de p√°ginas sin cargar im√°genes
    try:
        # Usar PyMuPDF para contar p√°ginas (muy eficiente en memoria)
        if PYMUPDF_AVAILABLE:
            import fitz
            pdf_doc = fitz.open(stream=pdf_content, filetype="pdf")
            total_pages = len(pdf_doc)
            pdf_doc.close()
        else:
            # Fallback: convertir solo primera p√°gina para contar
            first_page = convert_from_bytes(pdf_content, dpi=72, first_page=1, last_page=1)
            total_pages = len(convert_from_bytes(pdf_content, dpi=72))
            del first_page
            gc.collect()
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error contando p√°ginas: {e}")
        total_pages = 0
    
    # Si el PDF es muy grande (>100 p√°ginas), usar DPI m√°s bajo o fallback
    # ‚úÖ MEJORADO: DPI m√°s alto para mejor calidad de OCR
    if total_pages > 100:
        print(f"   ‚ö†Ô∏è PDF muy grande ({total_pages} p√°gs), usando DPI 150 para ahorrar memoria")
        dpi = 150
    elif total_pages > 50:
        print(f"   üìÑ PDF mediano ({total_pages} p√°gs), usando DPI 200")
        dpi = 200
    else:
        print(f"   üìÑ PDF peque√±o ({total_pages} p√°gs), usando DPI 300 (alta calidad)")
        dpi = 300
    
    text = ""
    error_count = 0
    processed_pages = 0
    
    # Procesar p√°gina por p√°gina para ahorrar memoria
    try:
        for page_num in range(1, total_pages + 1):
            try:
                # Convertir SOLO esta p√°gina a imagen
                images = convert_from_bytes(
                    pdf_content, 
                    dpi=dpi, 
                    first_page=page_num, 
                    last_page=page_num,
                    grayscale=True,  # Menos memoria
                    thread_count=1   # Menos memoria
                )
                
                if images:
                    # Aplicar OCR con idioma espa√±ol
                    page_text = pytesseract.image_to_string(images[0], lang='spa+eng')
                    
                    # Contar posibles errores
                    error_count += len(re.findall(r'[a-z]{3,}[A-Z][a-z]{2,}', page_text))
                    error_count += len(re.findall(r'\b\w{1,2}\s+\w{1,2}\s+\w{1,2}\b', page_text))
                    
                    text += page_text + "\n\n"
                    processed_pages += 1
                    
                    # Liberar memoria inmediatamente
                    del images
                    del page_text
                
                # Log de progreso cada 10 p√°ginas
                if page_num % 10 == 0:
                    print(f"   OCR p√°gina {page_num}/{total_pages}...")
                    gc.collect()  # Forzar limpieza de memoria
                    
            except Exception as page_error:
                print(f"   ‚ö†Ô∏è Error en p√°gina {page_num}: {page_error}")
                continue
                
    except Exception as e:
        print(f"   ‚ùå Error general en Tesseract: {e}")
        if processed_pages == 0:
            raise e
    
    # Limpieza final
    gc.collect()
    
    print(f"   ‚úÖ Tesseract completado: {len(text)} caracteres de {processed_pages} p√°ginas")
    return text.strip(), processed_pages if processed_pages > 0 else total_pages, error_count


def extract_with_pymupdf(pdf_content: bytes) -> Tuple[str, int, int]:
    """Extrae texto con PyMuPDF"""
    pdf_document = fitz.open(stream=pdf_content, filetype="pdf")
    text = ""
    total_pages = len(pdf_document)
    error_count = 0
    
    for page in pdf_document:
        page_text = page.get_text("text")
        # Contar errores de OCR (palabras pegadas o fragmentadas)
        error_count += len(re.findall(r'[a-z]{3,}[A-Z][a-z]{2,}', page_text))  # palabrasPegadas
        error_count += len(re.findall(r'\b\w{1,2}\s+\w{1,2}\s+\w{1,2}\b', page_text))  # f ra g men tos
        text += page_text + "\n\n"
    
    pdf_document.close()
    return text.strip(), total_pages, error_count


def detect_corrupted_text(text: str) -> Tuple[bool, str]:
    """
    Detecta si el texto extra√≠do est√° corrupto (encoding incorrecto, fuentes propietarias, etc.)
    
    Args:
        text: Texto extra√≠do del PDF
        
    Returns:
        Tuple[bool, str]: (est√°_corrupto, raz√≥n)
    """
    if not text or len(text) < 100:
        return True, "Texto muy corto o vac√≠o"
    
    # 1. Detectar caracteres de control o no imprimibles
    control_chars = len(re.findall(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', text))
    if control_chars > len(text) * 0.01:  # M√°s de 1% caracteres de control
        return True, f"Demasiados caracteres de control ({control_chars})"
    
    # 2. Detectar secuencias de s√≠mbolos que indican encoding incorrecto
    # Ej: "‚Üí", "‚Üê", "‚Üî", "‚áí", etc. que aparecen en medio de palabras
    arrow_in_words = len(re.findall(r'\w[‚Üí‚Üê‚Üî‚áí‚áê‚Üë‚Üì]\w', text))
    if arrow_in_words > 5:
        return True, f"S√≠mbolos de flecha en palabras ({arrow_in_words})"
    
    # 3. Detectar ratio bajo de vocales (texto normal tiene ~40% vocales en espa√±ol)
    vowels = len(re.findall(r'[aeiou√°√©√≠√≥√∫AEIOU√Å√â√ç√ì√ö]', text))
    letters = len(re.findall(r'[a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë]', text))
    if letters > 0:
        vowel_ratio = vowels / letters
        if vowel_ratio < 0.25:  # Menos de 25% vocales = probablemente corrupto
            return True, f"Ratio de vocales muy bajo ({vowel_ratio:.1%})"
    
    # 4. Detectar palabras con mezcla inusual de may√∫sculas/min√∫sculas
    # Ej: "grofeso", "entradJ", "aToda"
    weird_case = len(re.findall(r'\b[a-z]+[A-Z][a-z]*\b', text))
    if weird_case > len(text.split()) * 0.05:  # M√°s de 5% de palabras
        return True, f"Mezcla inusual de may√∫sculas ({weird_case} palabras)"
    
    # 5. Detectar secuencias de caracteres raros consecutivos
    # Ej: ")El", "J‚Üí", etc.
    weird_sequences = len(re.findall(r'[)}\]>][A-Za-z]|[A-Za-z][(\[{<]', text))
    if weird_sequences > 20:
        return True, f"Secuencias de caracteres raros ({weird_sequences})"
    
    # 6. Detectar palabras que no parecen espa√±ol/ingl√©s
    # Palabras de >4 letras sin vocales
    no_vowel_words = len(re.findall(r'\b[bcdfghjklmnpqrstvwxyzBCDFGHJKLMNPQRSTVWXYZ]{5,}\b', text))
    if no_vowel_words > 10:
        return True, f"Palabras sin vocales ({no_vowel_words})"
    
    # 7. ‚úÖ NUEVO: Detectar espacios insertados incorrectamente dentro de palabras
    # Patr√≥n: letra espacio 1-2 letras espacio letra (ej: "d e l a" o "rein a")
    fragmented_words = len(re.findall(r'\b[a-z√°√©√≠√≥√∫]\s[a-z√°√©√≠√≥√∫]{1,2}\s[a-z√°√©√≠√≥√∫]', text.lower()))
    if fragmented_words > 20:
        return True, f"Palabras fragmentadas por espacios ({fragmented_words})"
    
    # 8. ‚úÖ NUEVO: Detectar palabras pegadas sin espacios
    # Patr√≥n: secuencias muy largas de letras min√∫sculas (>25 caracteres sin espacio)
    glued_words = len(re.findall(r'[a-z√°√©√≠√≥√∫]{25,}', text.lower()))
    if glued_words > 10:
        return True, f"Palabras pegadas sin espacios ({glued_words})"
    
    # 9. ‚úÖ NUEVO: Detectar patr√≥n espec√≠fico de espaciado incorrecto
    # "del a" en lugar de "de la", "enwww" en lugar de "en www"
    bad_spacing_patterns = len(re.findall(r'\b(de|en|la|el|un|los|las|por|con)\s[a-z]\s', text.lower()))
    bad_spacing_patterns += len(re.findall(r'[a-z](www|http|com|org|net)', text.lower()))
    if bad_spacing_patterns > 15:
        return True, f"Espaciado incorrecto detectado ({bad_spacing_patterns})"
    
    return False, "Texto parece normal"


def repair_corrupted_spacing(text: str) -> str:
    """
    Repara texto con espaciado corrupto de PDFs con fuentes propietarias
    
    ‚úÖ ESTRATEGIA AGRESIVA:
    1. Primero: Unir fragmentos de palabras (quitar espacios incorrectos)
    2. Segundo: Separar palabras pegadas
    3. Tercero: Correcciones espec√≠ficas
    """
    if not text or len(text) < 50:
        return text
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 1: UNIR FRAGMENTOS DE PALABRAS
    # Patr√≥n: letra(s) + espacio + 1-2 letras + espacio/fin
    # Ejemplo: "rein a" ‚Üí "reina", "histori a" ‚Üí "historia"
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Unir fragmentos: "palabra a" o "palabr a" ‚Üí "palabraa" o "palabra"
    for _ in range(10):  # M√∫ltiples pasadas para casos anidados
        old_text = text
        # Patr√≥n: palabra + espacio + 1-2 letras al final
        text = re.sub(r'(\b\w{2,})\s+([a-z√°√©√≠√≥√∫√±]{1,2})\b', r'\1\2', text)
        # Patr√≥n: 1-2 letras + espacio + palabra
        text = re.sub(r'\b([a-z√°√©√≠√≥√∫√±]{1,2})\s+(\w{2,}\b)', r'\1\2', text)
        if text == old_text:
            break
    
    # Casos especiales de fragmentaci√≥n
    text = re.sub(r'\bdel\s+a\s+', 'de la ', text)  # "del a " ‚Üí "de la "
    text = re.sub(r'\bEL\s+EJANDRI\s+A\b', 'ALEJANDR√çA', text, flags=re.IGNORECASE)
    text = re.sub(r'\bel\s+ejandri\s+a\b', 'alejandr√≠a', text, flags=re.IGNORECASE)
    text = re.sub(r'\bA\s+ustria\b', 'Austria', text)
    text = re.sub(r'\bcon\s+de\s+sa\b', 'condesa', text, flags=re.IGNORECASE)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 2: SEPARAR PALABRAS PEGADAS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Art√≠culos pegados
    articles = ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'al', 'del']
    for art in articles:
        # "losbailes" ‚Üí "los bailes"
        text = re.sub(rf'\b({art})([a-z√°√©√≠√≥√∫√±]{{3,}})', rf'\1 \2', text, flags=re.IGNORECASE)
    
    # Preposiciones pegadas
    preps = ['con', 'en', 'de', 'por', 'para', 'sin', 'sobre', 'entre', 'hasta', 'desde', 'como', 'que']
    for prep in preps:
        text = re.sub(rf'\b({prep})([a-z√°√©√≠√≥√∫√±]{{3,}})', rf'\1 \2', text, flags=re.IGNORECASE)
    
    # Patrones espec√≠ficos de palabras pegadas
    pegadas = [
        (r'\bdeobras\b', 'de obras'),
        (r'\bdominio\s*p√∫blico\b', 'dominio p√∫blico'),
        (r'\bELcollar\b', 'El collar'),
        (r'\bMaurice\s*LEBLANC\b', 'Maurice Leblanc'),
        (r'\bcomolos\b', 'como los'),
        (r'\bDosotresveces\b', 'Dos o tres veces'),
        (r'\bsolemnidades\s*importantes\b', 'solemnidades importantes'),
        (r'\bluc√≠asobresus\b', 'luc√≠a sobre sus'),
        (r'\bblancos\s*hombros\b', 'blancos hombros'),
        (r'\bmaravilloso\s*collar\b', 'maravilloso collar'),
        (r'\bembajad\s*ade\b', 'embajada de'),
        (r'\bveladas\s*delady\b', 'veladas de lady'),
        (r'\bconmotivo\b', 'con motivo'),
        (r'\bala√±o\b', 'al a√±o'),
        (r'\benefecto\b', 'en efecto'),
        (r'\btalcomo\b', 'tal como'),
        (r'\bdurante\s*casi\b', 'durante casi'),
        (r'\bcasiunsiglo\b', 'casi un siglo'),
        (r'\btrendevida\b', 'tren de vida'),
        (r'\bantesque\b', 'antes que'),
        (r'\benajenar\b', 'enajenar'),
        (r'\bLIBRO\s*DE\s*SCARGADO\b', 'Libro descargado', re.IGNORECASE),
        (r'\bTUSITIOWEB\b', 'tu sitio web'),
        (r'\bESPERAMOSQUELO\b', 'Esperamos que lo'),
        (r'\bDISFRUT√âIS\b', 'disfrut√©is'),
        (r'\bPROJECT\s*GUTENBERG\b', 'Project Gutenberg'),
    ]
    
    for pattern, replacement in pegadas:
        if len(pattern) > 2:  # Evitar patrones muy cortos
            try:
                text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
            except:
                pass
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 3: LIMPIEZA FINAL
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # M√∫ltiples espacios ‚Üí uno solo
    text = re.sub(r'[ \t]+', ' ', text)
    
    # Espacios antes de puntuaci√≥n
    text = re.sub(r'\s+([.,;:!?])', r'\1', text)
    
    # Espacio despu√©s de puntuaci√≥n
    text = re.sub(r'([.,;:!?])([a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë¬ø¬°])', r'\1 \2', text)
    
    return text


def extract_with_pypdf2(pdf_content: bytes) -> Tuple[str, int, int]:
    """Extrae texto con PyPDF2"""
    pdf_file = BytesIO(pdf_content)
    pdf_reader = PyPDF2.PdfReader(pdf_file)
    text = ""
    total_pages = len(pdf_reader.pages)
    error_count = 0
    
    for page in pdf_reader.pages:
        page_text = page.extract_text() or ""
        error_count += len(re.findall(r'[a-z]{3,}[A-Z][a-z]{2,}', page_text))
        error_count += len(re.findall(r'\b\w{1,2}\s+\w{1,2}\s+\w{1,2}\b', page_text))
        text += page_text + "\n"
    
    return text.strip(), total_pages, error_count


def extract_text_from_pdf(pdf_content: bytes) -> tuple[str, int]:
    """
    Extrae texto de un archivo PDF usando el mejor m√©todo disponible
    
    ‚úÖ ESTRATEGIA REVISADA - TESSERACT PRIMERO:
    
    El problema: PDFs con fuentes propietarias producen texto corrupto con PyMuPDF
    porque las fuentes tienen mapeo de caracteres incorrecto.
    
    Soluci√≥n: SIEMPRE usar Tesseract OCR primero (lee la imagen visual),
    solo usar PyMuPDF como fallback si Tesseract falla.
    
    Args:
        pdf_content: Contenido del PDF en bytes
        
    Returns:
        tuple: (texto extra√≠do, n√∫mero total de p√°ginas)
    """
    import gc
    
    results = []
    total_pages = 0
    
    print(f"üìñ Extrayendo texto del PDF...")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 1: Contar p√°ginas primero (r√°pido con PyMuPDF)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if PYMUPDF_AVAILABLE:
        try:
            import fitz
            pdf_doc = fitz.open(stream=pdf_content, filetype="pdf")
            total_pages = len(pdf_doc)
            pdf_doc.close()
            print(f"   üìÑ PDF tiene {total_pages} p√°ginas")
        except Exception as e:
            print(f"   ‚ö†Ô∏è No se pudo contar p√°ginas: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 2: TESSERACT OCR PRIMERO (mejor calidad, lee imagen visual)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if TESSERACT_AVAILABLE:
        try:
            print(f"   üîç Usando Tesseract OCR (lee imagen visual del PDF)...")
            text_tess, pages_tess, errors_tess = extract_with_tesseract(pdf_content)
            
            # Verificar que Tesseract produjo texto v√°lido
            if len(text_tess.strip()) > 100:
                is_corrupted, reason = detect_corrupted_text(text_tess)
                if not is_corrupted:
                    results.append(('Tesseract', text_tess, pages_tess, errors_tess))
                    total_pages = pages_tess
                    print(f"   ‚úÖ Tesseract: {len(text_tess)} chars, texto limpio")
                    
                    # Usar Tesseract directamente
                    text = aggressive_text_cleanup(text_tess)
                    gc.collect()
                    print(f"‚úÖ Texto extra√≠do con OCR: {len(text)} caracteres de {pages_tess} p√°ginas")
                    return text, pages_tess
                else:
                    print(f"   ‚ö†Ô∏è Tesseract produjo texto con problemas: {reason}")
                    results.append(('Tesseract', text_tess, pages_tess, errors_tess))
            else:
                print(f"   ‚ö†Ô∏è Tesseract produjo muy poco texto ({len(text_tess)} chars)")
                
        except Exception as e:
            print(f"   ‚ùå Tesseract fall√≥: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 3: PyMuPDF como FALLBACK (solo si Tesseract fall√≥)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if PYMUPDF_AVAILABLE and not results:
        try:
            print(f"   üìñ Fallback a PyMuPDF...")
            text_mupdf, pages_mupdf, errors_mupdf = extract_with_pymupdf(pdf_content)
            results.append(('PyMuPDF', text_mupdf, pages_mupdf, errors_mupdf))
            total_pages = pages_mupdf
            print(f"   PyMuPDF: {len(text_mupdf)} chars, {errors_mupdf} errores")
        except Exception as e:
            print(f"   ‚ùå PyMuPDF fall√≥: {e}")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 4: PyPDF2 como √∫ltimo recurso
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    if not results and PYPDF2_AVAILABLE:
        try:
            print(f"   üìÑ √öltimo recurso: PyPDF2...")
            text_pypdf2, pages_pypdf2, errors_pypdf2 = extract_with_pypdf2(pdf_content)
            results.append(('PyPDF2', text_pypdf2, pages_pypdf2, errors_pypdf2))
            total_pages = pages_pypdf2
            print(f"   PyPDF2: {len(text_pypdf2)} chars")
        except Exception as e:
            print(f"   ‚ùå PyPDF2 fall√≥: {e}")
    
    if not results:
        raise Exception("No se pudo extraer texto del PDF con ning√∫n m√©todo")
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # ELEGIR EL MEJOR RESULTADO (el de menos errores)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    best = min(results, key=lambda x: x[3])  # x[3] = error_count
    print(f"   ‚úÖ Usando {best[0]}")
    
    text = best[1]
    total_pages = best[2]
    
    # Limpiar texto extra√≠do
    text = aggressive_text_cleanup(text)
    
    # Limpiar memoria
    gc.collect()
    
    print(f"‚úÖ Texto extra√≠do: {len(text)} caracteres de {total_pages} p√°ginas")
    return text, total_pages


def aggressive_text_cleanup(text: str) -> str:
    """
    Limpieza AGRESIVA de texto extra√≠do de PDF
    
    ‚úÖ MEJORADO: Repara texto con espaciado incorrecto de PyMuPDF
    
    Problema detectado: PDFs con fuentes propietarias producen:
    - "de scargadoenwww" ‚Üí "descargado en www"
    - "el ejandri a" ‚Üí "alejandr√≠a"
    - "lacon desade" ‚Üí "la condesa de"
    """
    if not text:
        return text
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 0: Reparaci√≥n de espaciado corrupto (NUEVO)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    text = repair_corrupted_spacing(text)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 1: Separar palabras pegadas (camelCase accidental)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # "serreconocido" ‚Üí "ser reconocido" (min√∫scula seguida de may√∫scula)
    text = re.sub(r'([a-z√°√©√≠√≥√∫√±])([A-Z√Å√â√ç√ì√ö√ë])', r'\1 \2', text)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 2: Unir fragmentos sueltos (errores OCR t√≠picos)
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Patr√≥n: "palabra + espacio + 1-3 letras" ‚Üí unir
    # Ej: "histori a" ‚Üí "historia", "Henriet te" ‚Üí "Henriette"
    for _ in range(5):  # Repetir varias veces para casos anidados
        text = re.sub(r'(\w{3,})\s+([a-z√°√©√≠√≥√∫√±]{1,3})\b', r'\1\2', text, flags=re.IGNORECASE)
    
    # Patr√≥n: "1-4 letras + espacio + palabra" ‚Üí unir
    # Ej: "a doptar" ‚Üí "adoptar"
    for _ in range(3):
        text = re.sub(r'\b([a-z√°√©√≠√≥√∫√±]{1,4})\s+([a-z√°√©√≠√≥√∫√±]{3,})', r'\1\2', text, flags=re.IGNORECASE)
    
    # Patr√≥n: May√∫scula + espacio + resto
    # Ej: "V alorbe" ‚Üí "Valorbe"
    text = re.sub(r'\b([A-Z√Å√â√ç√ì√ö√ë])\s+([a-z√°√©√≠√≥√∫√±]{2,})', r'\1\2', text)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 3: Separar palabras que deber√≠an estar separadas
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Art√≠culos pegados a palabras
    articles = ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas', 'al', 'del']
    for art in articles:
        # "losdem√°s" ‚Üí "los dem√°s"
        text = re.sub(rf'\b({art})([a-z√°√©√≠√≥√∫√±]{{3,}})', rf'\1 \2', text, flags=re.IGNORECASE)
    
    # Preposiciones pegadas
    preps = ['con', 'en', 'de', 'por', 'para', 'sin', 'sobre', 'entre', 'hasta', 'desde', 'como', 'que']
    for prep in preps:
        text = re.sub(rf'\b({prep})([a-z√°√©√≠√≥√∫√±]{{3,}})', rf'\1 \2', text, flags=re.IGNORECASE)
    
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # PASO 4: Limpiar puntuaci√≥n y espacios
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    # Unir palabras cortadas por gui√≥n al final de l√≠nea
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text)
    
    # M√∫ltiples espacios ‚Üí uno solo
    text = re.sub(r'[ \t]+', ' ', text)
    
    # M√∫ltiples saltos de l√≠nea ‚Üí m√°ximo 2
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # Espacios antes de puntuaci√≥n
    text = re.sub(r'\s+([.,;:!?])', r'\1', text)
    
    # Espacio despu√©s de puntuaci√≥n si falta
    text = re.sub(r'([.,;:!?])([a-z√°√©√≠√≥√∫√±A-Z√Å√â√ç√ì√ö√ë¬ø¬°])', r'\1 \2', text)
    
    # Remover l√≠neas que solo tienen n√∫meros (paginaci√≥n)
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
    
    return text.strip()

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """
    Divide el texto en chunks con overlap para mantener contexto
    
    OPTIMIZADO PARA PDFs DE 25-100+ P√ÅGINAS:
    - chunk_size=1000: Contexto completo (5-7 oraciones)
    - overlap=200: Mayor continuidad entre chunks
    
    Args:
        text: Texto a dividir
        chunk_size: Tama√±o aproximado de cada chunk (en caracteres) [default: 1000]
        overlap: Cantidad de caracteres que se solapan entre chunks [default: 200]
        
    Returns:
        List[str]: Lista de chunks de texto
    """
    # Limpiar el texto
    text = clean_text(text)
    
    # Dividir por oraciones
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        # Si agregar esta oraci√≥n excede el tama√±o, guardar el chunk actual
        if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            
            # Comenzar nuevo chunk con overlap
            words = current_chunk.split()
            overlap_words = min(overlap, len(words))
            overlap_text = ' '.join(words[-overlap_words:]) if overlap_words > 0 else ""
            current_chunk = overlap_text + " " + sentence if overlap_text else sentence
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    # Agregar el √∫ltimo chunk
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # ‚úÖ NUEVO: Normalizar todos los chunks para corregir errores OCR
    if NORMALIZER_AVAILABLE:
        chunks = [normalize_text(chunk) for chunk in chunks]
        print(f"‚úÖ Chunks normalizados: errores OCR corregidos")
    
    return chunks

def clean_text(text: str) -> str:
    """
    Limpia el texto removiendo caracteres innecesarios
    
    NOTA IMPORTANTE: 
    - Si el PDF tiene OCR defectuoso (espacios en medio de palabras), 
      este filtro NO lo arreglar√° autom√°ticamente
    - Para PDFs con OCR corrupto, ejecutar manualmente el script SQL:
      database/fix_ocr_chunks_CORRECTO.sql
    
    Args:
        text: Texto a limpiar
        
    Returns:
        str: Texto limpio
    """
    # Remover m√∫ltiples espacios
    text = re.sub(r'\s+', ' ', text)
    
    # Remover caracteres especiales pero mantener puntuaci√≥n b√°sica
    text = re.sub(r'[^\w\s.,;:!?¬ø¬°√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë()"\'-]', '', text)
    
    # Remover l√≠neas vac√≠as m√∫ltiples
    text = re.sub(r'\n\s*\n', '\n', text)
    
    return text.strip()

def get_text_stats(text: str, real_pages: int = None) -> dict:
    """
    Obtiene estad√≠sticas del texto
    
    Args:
        text: Texto a analizar
        real_pages: N√∫mero real de p√°ginas del PDF (si est√° disponible)
        
    Returns:
        dict: Diccionario con estad√≠sticas
    """
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    paragraphs = text.split('\n\n')
    
    # Calcular p√°ginas estimadas bas√°ndose en caracteres (1300 chars/p√°gina es m√°s realista)
    estimated_pages = len(text) // 1300 if not real_pages else real_pages
    
    return {
        "characters": len(text),
        "characters_no_spaces": len(text.replace(' ', '')),
        "words": len(words),
        "sentences": len([s for s in sentences if s.strip()]),
        "paragraphs": len([p for p in paragraphs if p.strip()]),
        "avg_word_length": round(sum(len(word) for word in words) / len(words), 2) if words else 0,
        "avg_sentence_length": round(len(words) / len([s for s in sentences if s.strip()]), 2) if sentences else 0,
        "estimated_pages": estimated_pages,
        "real_pages": real_pages if real_pages else estimated_pages  # Devolver el conteo real si existe
    }

def chunk_by_paragraphs(text: str, max_chunk_size: int = 1000) -> List[str]:
    """
    Divide el texto en chunks bas√°ndose en p√°rrafos
    
    Args:
        text: Texto a dividir
        max_chunk_size: Tama√±o m√°ximo de cada chunk
        
    Returns:
        List[str]: Lista de chunks
    """
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph:
            continue
        
        if len(current_chunk) + len(paragraph) > max_chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = paragraph
        else:
            current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

def smart_chunk(text: str, target_size: int = 500, min_size: int = 100) -> List[str]:
    """
    Chunking inteligente que respeta l√≠mites de oraciones y p√°rrafos
    
    Args:
        text: Texto a dividir
        target_size: Tama√±o objetivo de cada chunk
        min_size: Tama√±o m√≠nimo aceptable
        
    Returns:
        List[str]: Lista de chunks optimizados
    """
    text = clean_text(text)
    
    # Dividir por p√°rrafos primero
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = ""
    
    for paragraph in paragraphs:
        # Si el p√°rrafo es muy grande, dividirlo por oraciones
        if len(paragraph) > target_size:
            sentences = re.split(r'(?<=[.!?])\s+', paragraph)
            
            for sentence in sentences:
                if len(current_chunk) + len(sentence) > target_size and len(current_chunk) >= min_size:
                    chunks.append(current_chunk.strip())
                    current_chunk = sentence
                else:
                    current_chunk += " " + sentence if current_chunk else sentence
        else:
            # Agregar p√°rrafo completo
            if len(current_chunk) + len(paragraph) > target_size and len(current_chunk) >= min_size:
                chunks.append(current_chunk.strip())
                current_chunk = paragraph
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
    
    # Agregar el √∫ltimo chunk
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    # ‚úÖ NUEVO: Normalizar todos los chunks para corregir errores OCR
    if NORMALIZER_AVAILABLE:
        print(f"üßπ Normalizando {len(chunks)} chunks (corrigiendo errores OCR)...")
        chunks = [normalize_text(chunk) for chunk in chunks]
        print(f"‚úÖ Chunks normalizados correctamente")
    
    return chunks


def adaptive_chunking(text: str, total_pages: int) -> List[str]:
    """
    üéØ CHUNKING ADAPTATIVO INTELIGENTE seg√∫n tama√±o del PDF
    
    Ajusta autom√°ticamente los par√°metros de chunking para mantener
    el equilibrio entre precisi√≥n y eficiencia seg√∫n el tama√±o del documento.
    
    ESTRATEGIA POR TAMA√ëO:
    üìò PDFs peque√±os (1-50 p√°gs):    chunks detallados (80-180 palabras)
    üìó PDFs medianos (51-300 p√°gs):  chunks moderados (150-350 palabras)
    üìï PDFs grandes (301-1000 p√°gs): chunks amplios (250-600 palabras)
    üìö PDFs masivos (1000+ p√°gs):    chunks extensos (400-1000 palabras)
    
    BENEFICIOS:
    ‚úÖ Reduce ruido en PDFs grandes (menos chunks = mejor retrieval)
    ‚úÖ Mantiene detalle en PDFs peque√±os
    ‚úÖ Optimiza tiempo de procesamiento
    ‚úÖ Mejor balance precisi√≥n/escalabilidad
    
    Args:
        text: Texto completo a dividir
        total_pages: N√∫mero total de p√°ginas del PDF
        
    Returns:
        List[str]: Chunks optimizados seg√∫n tama√±o del documento
    """
    print(f"\nüéØ CHUNKING ADAPTATIVO para PDF de {total_pages} p√°ginas")
    
    if total_pages <= 50:
        # PDFs peque√±os: m√°ximo detalle
        print("   üìò Estrategia: DETALLADA (80-180 palabras/chunk)")
        return semantic_chunking(text, min_words=80, max_words=180, overlap_words=20)
    
    elif total_pages <= 300:
        # PDFs medianos: balance detalle/eficiencia
        print("   üìó Estrategia: MODERADA (150-350 palabras/chunk)")
        return semantic_chunking(text, min_words=150, max_words=350, overlap_words=30)
    
    elif total_pages <= 1000:
        # PDFs grandes: priorizar coherencia
        print("   üìï Estrategia: AMPLIA (250-600 palabras/chunk)")
        return semantic_chunking(text, min_words=250, max_words=600, overlap_words=50)
    
    else:
        # PDFs masivos: reducir ruido al m√°ximo
        print("   üìö Estrategia: EXTENSIVA (400-1000 palabras/chunk)")
        return semantic_chunking(text, min_words=400, max_words=1000, overlap_words=80)

def semantic_chunking(text: str, min_words: int = 150, max_words: int = 400, overlap_words: int = 15) -> List[str]:
    """
    üß† CHUNKING SEM√ÅNTICO INTELIGENTE - BASE
    
    Divide texto por P√ÅRRAFOS Y ORACIONES (no caracteres arbitrarios).
    Se adapta al contenido respetando l√≠mites sem√°nticos.
    
    CARACTER√çSTICAS:
    ‚úÖ Divisi√≥n por p√°rrafos (\n\n) - respeta estructura del documento
    ‚úÖ Subdivisi√≥n por oraciones si p√°rrafo es muy largo
    ‚úÖ Context anchors: 15 palabras de overlap entre chunks
    ‚úÖ Rango adaptativo: 150-400 palabras (no caracteres fijos)
    ‚úÖ Respeta l√≠mites de ideas completas
    
    EJEMPLO DE RESULTADO:
    - Chunk antiguo (1000 chars): "...una amiga de convento que se enemis..." (cortado)
    - Chunk sem√°ntico (250 palabras): "En el edificio viv√≠a una amiga de convento que se enemist√≥ 
      con su familia. Prestaba servicios a la condesa y conoc√≠a sus rutinas. Siempre se hablaba 
      delante de ella. Su ventana de cocina daba exactamente al mismo patio interior..." (completo)
    
    Args:
        text: Texto completo a dividir
        min_words: M√≠nimo de palabras por chunk (default: 150)
        max_words: M√°ximo de palabras por chunk (default: 400)
        overlap_words: Palabras de overlap entre chunks (default: 15)
        
    Returns:
        List[str]: Chunks sem√°nticos con context anchors
    """
    text = clean_text(text)
    
    # 1. DIVIDIR POR P√ÅRRAFOS (respeta estructura del documento)
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    chunks = []
    current_chunk = []
    word_count = 0
    
    print(f"\nüß† INICIANDO CHUNKING SEM√ÅNTICO...")
    print(f"   Rango: {min_words}-{max_words} palabras por chunk")
    print(f"   Context anchors: {overlap_words} palabras de overlap")
    
    for paragraph in paragraphs:
        paragraph_words = paragraph.split()
        paragraph_word_count = len(paragraph_words)
        
        # Si el p√°rrafo es muy largo (> max_words), dividirlo por oraciones
        if paragraph_word_count > max_words:
            sentences = re.split(r'(?<=[.!?])\s+', paragraph)
            
            for sentence in sentences:
                sentence_words = sentence.split()
                sentence_word_count = len(sentence_words)
                
                # Si agregar esta oraci√≥n supera max_words, guardar chunk actual
                if word_count + sentence_word_count > max_words and word_count >= min_words:
                    # Guardar chunk actual
                    chunk_text = ' '.join(current_chunk)
                    chunks.append(chunk_text)
                    
                    # Context anchor: √∫ltimas N palabras del chunk anterior
                    overlap = current_chunk[-overlap_words:] if len(current_chunk) >= overlap_words else current_chunk
                    current_chunk = overlap + sentence_words
                    word_count = len(current_chunk)
                else:
                    current_chunk.extend(sentence_words)
                    word_count += sentence_word_count
        else:
            # P√°rrafo completo cabe en el chunk actual
            if word_count + paragraph_word_count > max_words and word_count >= min_words:
                # Guardar chunk actual
                chunk_text = ' '.join(current_chunk)
                chunks.append(chunk_text)
                
                # Context anchor
                overlap = current_chunk[-overlap_words:] if len(current_chunk) >= overlap_words else current_chunk
                current_chunk = overlap + paragraph_words
                word_count = len(current_chunk)
            else:
                # Agregar p√°rrafo al chunk actual
                if current_chunk:
                    current_chunk.append('\n\n')
                current_chunk.extend(paragraph_words)
                word_count += paragraph_word_count
    
    # Agregar √∫ltimo chunk
    if current_chunk:
        chunk_text = ' '.join(current_chunk)
        chunks.append(chunk_text)
    
    # Normalizar chunks
    if NORMALIZER_AVAILABLE:
        print(f"üßπ Normalizando {len(chunks)} chunks sem√°nticos...")
        chunks = [normalize_text(chunk) for chunk in chunks]
    
    # Estad√≠sticas
    chunk_lengths = [len(chunk.split()) for chunk in chunks]
    avg_words = sum(chunk_lengths) / len(chunks) if chunks else 0
    
    print(f"\n‚úÖ CHUNKING SEM√ÅNTICO COMPLETADO:")
    print(f"   Total chunks: {len(chunks)}")
    print(f"   Promedio palabras/chunk: {avg_words:.1f}")
    print(f"   Rango: {min(chunk_lengths)}-{max(chunk_lengths)} palabras")
    print(f"   Context anchors: {overlap_words} palabras de overlap\n")
    
    return chunks
